
📥 Loading file: ../debug_wavs/sample_00.wav
🔍 [pydub] Raw frame_rate   : 16000
🔍 [pydub] Sample width     : 4 bytes (32 bits)
🔍 [pydub] Channels         : 1
🔍 [pydub] Duration (ms)    : 2375 ms
🧪 [pydub] Type of array     : <class 'array.array'>, dtype: int16
🧪 [pydub] First 10 samples  : array('h', [0, 0, -1, 0, 0, 0, 1, 3, 9, 11])
✅ [pydub] Waveform shape    : torch.Size([1, 38000])
📊 [pydub] Min: -22944.00, Max: 24431.00, Mean: -0.01

🔁 [Compare] Loading with torchaudio.load()
✅ [torchaudio] shape        : torch.Size([1, 38000]), sample_rate: 16000
📊 [torchaudio] Min: -0.7002, Max: 0.7456, Mean: -0.0000
📏 Diff (mean abs): 0.0000 (assuming torchaudio gives normalized)
========== 🔧 CONFIG SUMMARY ==========
  chunk_size               = 64
  left_context_size        = 128
  right_context_size       = 128
  subsampling_factor       = 8
  conv_kernel_size         = 15
  conv_lorder              = 7
  num_blocks               = 17
  hidden_dim (_output_size)= 512
  attention_heads          = 8
  Input shape              = torch.Size([1, 236, 80])
=======================================

🧮 CALCULATED CONTEXT INFO
  total_batch_duration     = 1800
  max_len (frame count)    = 89999
  multiply_n               = 175
  truncated_context_size   = 11200
  rel_right_context_size   = 17408


📦 Chunk 0
  Input chunk frame idx: 0 → 236
  Input x shape         : torch.Size([1, 236, 80])
  x_len                 : [236]

================= 🧩 [Encoder.forward_parallel_chunk] START =================
📥 Input shape: torch.Size([1, 236, 80]), xs_origin_lens: [236]
⚙️ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200
📏 Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7
🔹 Sample 0: original_len=236, padded_len=519, pad_frames=283, n_chunks=1, offset=0

🧱 Total chunked xs shape: torch.Size([1, 519, 80])
📐 xs_lens (post chunk): torch.Size([1]), total_chunks: 1
✅ Applied Global CMVN
🎛️ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])
🧮 att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])
🧩 Layer 0: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 0
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 1: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 1
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 2: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 2
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 3: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 3
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 4: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 4
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 5: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 5
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 6: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 6
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 7: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 7
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 8: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 8
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 9: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 9
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 10: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 10
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 11: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 11
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 12: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 12
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 13: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 13
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 14: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 14
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 15: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 15
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
🧩 Layer 16: xs shape after layer = torch.Size([1, 64, 512])
	🧩 Layer 16
		ChunkFormerEncoderLayer(
  (self_attn): StreamingRelPositionMultiHeadedAttention(
    (linear_q): Linear(in_features=512, out_features=512, bias=True)
    (linear_k): Linear(in_features=512, out_features=512, bias=True)
    (linear_v): Linear(in_features=512, out_features=512, bias=True)
    (linear_out): Linear(in_features=512, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear_pos): Linear(in_features=512, out_features=512, bias=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (feed_forward_macaron): PositionwiseFeedForward(
    (w_1): Linear(in_features=512, out_features=2048, bias=True)
    (activation): SiLU()
    (dropout): Dropout(p=0.1, inplace=False)
    (w_2): Linear(in_features=2048, out_features=512, bias=True)
  )
  (conv_module): ConvolutionModule(
    (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
    (depthwise_conv): Conv1d(512, 512, kernel_size=(15,), stride=(1,), groups=512)
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
    (activation): SiLU()
  )
  (norm_ff): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_mha): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_ff_macaron): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (norm_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
📏 Applied LayerNorm after encoder
📤 Final offset: [28]

✅ [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [28], n_chunks: [1]
====================================================================

  encoder_outs shape    : torch.Size([1, 64, 512])
  encoder_lens          : [28]
  framewise_ids shape   : torch.Size([28])
  framewise_ids (first 10): [0, 0, 0, 0, 4657, 0, 0, 6324, 0, 0]

📊 Total full_framewise_ids shape: torch.Size([28])
    Sample token IDs (first 20): [0, 0, 0, 0, 4657, 0, 0, 6324, 0, 0, 5854, 0, 0, 6819, 0, 2983, 0, 0, 1635, 0]

📝 Decoded segments (first 3):
  → {'decode': ' nửa vòng trái đất hơn bảy năm', 'start': '00:00:00:000', 'end': '00:00:02:160'}

✅ Final transcript:  nửa vòng trái đất hơn bảy năm
🟢 Prediction     :  nửa vòng trái đất hơn bảy năm
🔵 Ground Truth   : nửa vòng trái đất hơn bảy năm
❌ WER            : 0.0
