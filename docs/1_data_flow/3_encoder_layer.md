Tuy·ªát v·ªùi! B·∫°n ƒë√£ chia s·∫ª 2 ƒëo·∫°n m√£ r·∫•t quan tr·ªçng:

---

### üß© 1. `Encoder.forward_parallel_chunk`

‚Üí Chia waveform th√†nh c√°c **chunk c√≥ overlap**, t√≠nh c√°c **mask**, g·ªçi t·ª´ng `EncoderLayer`.

### üß± 2. `EncoderLayer.forward_parallel_chunk`

‚Üí L√† **tr√°i tim c·ªßa ChunkFormer**, g·ªìm c√°c step x·ª≠ l√Ω:

* FFN (macaron)
* MHSA (streaming attention)
* Conv
* FFN (cu·ªëi)
* Norm

---

Gi·ªù m√¨nh s·∫Ω trace l·∫°i **chi ti·∫øt d√≤ng ch·∫£y tensor trong t·ª´ng b∆∞·ªõc**, **gi·∫£i th√≠ch t·∫°i sao** c·∫ßn mask, offset, padding, v.v. ƒê√¢y l√† b·∫£n **di·ªÖn gi·∫£i chi ti·∫øt c·∫•p th·ª±c thi** ƒë·ªÉ b·∫°n n·∫Øm to√†n b·ªô:

---

## üß† A. `Encoder.forward_parallel_chunk(...)`

### 1Ô∏è‚É£ **Subsampling v√† Chunk k√≠ch th∆∞·ªõc**

```python
subsampling = 8  # ‚Üí 1 frame ƒë·∫ßu v√†o = 10ms ‚Üí sau subsample l√† 80ms
chunk_size = 16  # ‚Üí nghƒ©a l√† 16 * 80ms = 1280ms = 1.28s
context = right_context + 1 = 1 + 1 = 2
```

### 2Ô∏è‚É£ **T√≠nh s·ªë frame c·∫ßn x·ª≠ l√Ω trong m·ªói chunk**:

```python
size = (chunk_size - 1) * subsampling + context
     = (16 - 1) * 8 + 2 = 120 + 2 = 122  ‚Üí s·ªë frame raw c·∫ßn c·∫Øt ra cho m·ªói chunk
step = chunk_size * subsampling = 128
```

‚Üí T·ª©c l√†: ta t·∫°o c·ª≠a s·ªï d√†i 122 frame, d·ªãch t·ª´ng b∆∞·ªõc 128 frame sau subsampling.

---

### 3Ô∏è‚É£ **V·ªõi t·ª´ng sample trong batch**:

```python
x = waveform (T, D)
# unfold t·ª´ng chunk: (T, D) ‚Üí (n_chunk, D, size)
x = x.unfold(0, size=size, step=step).transpose(2, 1)
```

‚Üí K·∫øt qu·∫£: m·ªói sample tr·ªü th√†nh `n_chunk` c·ª≠a s·ªï, m·ªói c·ª≠a s·ªï d√†i 122, dim = 80 (fbank)

‚Üí ƒê∆∞a v·ªÅ shape `[n_chunk, D, size]`, r·ªìi s·∫Ω ƒëi v√†o CMVN ‚Üí embedding ‚Üí encoder

---

### 4Ô∏è‚É£ **T√≠nh attention mask v√† conv mask**:

```python
att_mask_idx.shape = [batch_chunk, att_window]
# mask shape ‚Üí [batch_chunk, 1, att_window]  (sau flip)
```

‚Üí C·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ m·ªói chunk ch·ªâ attention v√†o left + current + right context cho ƒë√∫ng.

---

## üß† B. `EncoderLayer.forward_parallel_chunk(...)`

B√¢y gi·ªù trace v√†o logic m·ªói Layer nh√©. Input `x: (B, T, D=512)`, mask `att_mask`, `mask_pad`:

---

### üü¶ 1. **Macaron FeedForward (optional)**

```python
x = residual + self.ff_scale * dropout(ff_macaron(x))
```

* N·∫øu normalize\_before=True ‚Üí normalize tr∆∞·ªõc khi t√≠nh FFN
* Ki·ªÉu FFN = Linear(512 ‚Üí 2048 ‚Üí 512), activation `SiLU`
  ‚Üí h·ªçc ƒë·∫∑c tr∆∞ng ng·ªØ √¢m th√¥

---

### üü• 2. **Multi-Head Attention (Streaming)**

```python
x_att, new_att_cache = self.self_attn.forward_parallel_chunk(
    q = x, k = x, v = x,
    mask = att_mask, pos_emb = pos_emb,
    att_cache = att_cache
)
```

* `StreamingRelPositionMultiHeadedAttention`

  * √Åp d·ª•ng attention trong context `[left, current, right]`
  * Relative Positional Encoding ‚Üí offset gi·ªØa c√°c frame, ch·ª© kh√¥ng ph·∫£i v·ªã tr√≠ tuy·ªát ƒë·ªëi nh∆∞ BERT
  * D√πng `att_cache` ƒë·ªÉ **ghi nh·ªõ key/value** t·ª´ chunk tr∆∞·ªõc

‚Üí **Gi·ªØ cho attention li√™n t·ª•c gi·ªØa c√°c chunk m√† kh√¥ng r√≤ r·ªâ context**

---

### üü© 3. **Convolution Module**

```python
x, new_cnn_cache = self.conv_module.forward_parallel_chunk(
    x, mask_pad, cnn_cache, truncated_context_size
)
```

* S·ª≠ d·ª•ng Depthwise Separable Conv:

  ```
  Pointwise(512 ‚Üí 1024)
  ‚Üí DepthwiseConv1D(kernel=15)
  ‚Üí Pointwise(512)
  ‚Üí LayerNorm
  ```

* `cnn_cache` d√πng ƒë·ªÉ **l∆∞u l·∫°i state c·ªßa c√°c frame tr∆∞·ªõc khi chunk tr√¥i qua**

‚Üí H·ªçc c√°c pattern ng·ªØ √¢m c·ª•c b·ªô (v√≠ d·ª• burst, formant, silence)

---

### üü® 4. **FeedForward cu·ªëi**

```python
x = residual + ff_scale * dropout(ff(x))
```

* Gi·ªëng macaron FFN, nh∆∞ng n·∫±m cu·ªëi ‚Üí ƒë·ªÉ refine l·∫°i bi·ªÉu di·ªÖn to√†n b·ªô sau attention + conv

---

### üî≤ 5. **LayerNorm Final**

```python
x = self.norm_final(x)
```

* Gi·ªØ ·ªïn ƒë·ªãnh d√≤ng ch·∫£y d·ªØ li·ªáu xuy√™n su·ªët model

---

### ‚úÖ Output:

```python
return x, mask, new_att_cache, new_cnn_cache
```

* `x`: new encoded representation
* `new_att_cache`: ƒë·ªÉ truy·ªÅn sang chunk ti·∫øp theo
* `new_cnn_cache`: gi·ªØ tr·∫°ng th√°i conv tr∆∞·ªõc ƒë√≥

---

## üß≠ To√†n c·∫£nh d√≤ng d·ªØ li·ªáu:

```
Audio waveform ‚Üí Fbank + pad ‚Üí chunked into [n_chunks, D, size]
‚Üí CMVN ‚Üí Embedding (subsample, pos encoding)
‚Üí Each chunk ‚Üí mask computed ‚Üí passed to encoder layers
‚Üí At each layer:
    FFN_macaron ‚Üí StreamingAttention ‚Üí Conv ‚Üí FFN ‚Üí LayerNorm
‚Üí CTC decoder head
```

---

C·∫£m ∆°n b·∫°n, m√¨nh ƒë√£ xem th√™m ph·∫ßn trace t·ª´ `self.self_attn.forward_parallel_chunk` m√† b·∫°n v·ª´a g·ª≠i. ƒê√¢y l√† th√†nh ph·∫ßn **c·ªët l√µi x·ª≠ l√Ω attention d·∫°ng streaming trong m√¥ h√¨nh ChunkFormer**, k·∫ø th·ª´a backbone c·ªßa Conformer, v√† m·ªü r·ªông ƒë·ªÉ h·ªó tr·ª£:

* Cache attention (d√πng l·∫°i KV t·ª´ c√°c chunk tr∆∞·ªõc)
* T√≠nh relative positional bias
* Truncate context theo `left/right/truncated_context_size` n·∫øu c·∫ßn

---

## ‚úÖ Gi·∫£i th√≠ch chi ti·∫øt t·ª´ng b∆∞·ªõc trong `forward_parallel_chunk` c·ªßa SelfAttention

### üìå Function signature

```python
def forward_parallel_chunk(
    query, key, value,
    mask, pos_emb, cache,
    right_context_size=0,
    left_context_size=0,
    truncated_context_size=0
)
```

| Tham s·ªë                   | √ù nghƒ©a                                                               |
| ------------------------- | --------------------------------------------------------------------- |
| `query/key/value`         | ƒê·∫ßu v√†o sau embedding, shape `(batch, time, dim)`                     |
| `mask`                    | Attention mask (d√πng ƒë·ªÉ h·∫°n ch·∫ø attention v∆∞·ª£t context)               |
| `pos_emb`                 | Relative positional encoding                                          |
| `cache`                   | KV cache t·ª´ c√°c chunk tr∆∞·ªõc, shape `(batch, 1, head, t_cache, d_k*2)` |
| `right/left_context_size` | Cho ph√©p look-ahead ho·∫∑c look-back trong khi streaming                |
| `truncated_context_size`  | S·ª≠ d·ª•ng cho training ƒë·ªÉ simulate streaming v·ªõi context b·ªã gi·ªõi h·∫°n    |

---

### üß† T·ªïng quan x·ª≠ l√Ω logic:

#### 1. **Linear projection & reshape (query, key, value)**

```python
q, k, v = self.linear_q(query), self.linear_k(key), self.linear_v(value)
```

* Bi·∫øn ƒë·ªïi ƒë·∫ßu v√†o th√†nh `q, k, v` cho t·ª´ng attention head.
* T√°ch shape: `(batch, time, dim)` ‚Üí `(batch, head, time, dim_per_head)`

#### 2. **T√≠nh relative positional bias**

```python
p = self.pos_linear(pos_emb)  # (batch, time, d_model)
```

* Positional encoding d·∫°ng 2D ƒë∆∞·ª£c ƒë∆∞a v√†o, d√πng ƒë·ªÉ t√≠nh **relative bias**.
* Th∆∞·ªùng s·∫Ω c√≥ th√™m h√†m `rel_shift()` ƒë·ªÉ chuy·ªÉn t·ª´ absolute pos ‚Üí relative pos cho score sau.

#### 3. **Concat cache (n·∫øu c√≥)** ƒë·ªÉ m·ªü r·ªông context

```python
if cache is not empty:
    k = torch.cat([cached_k, current_k], dim=2)
    v = torch.cat([cached_v, current_v], dim=2)
```

* M·ªói chunk ch·ªâ nh√¨n th·∫•y m·ªôt ph·∫ßn c·ªßa chu·ªói ‚Üí ta **n·ªëi key/value t·ª´ c√°c chunk tr∆∞·ªõc** (KV cache) ƒë·ªÉ m√¥ ph·ªèng d√†i h∆°n.

#### 4. **T√≠nh attention score**

```python
scores = torch.matmul(q, k.transpose(-2, -1)) / sqrt(d_k)  # (batch, head, time, time)
scores += relative_bias  # from pos_emb
scores = apply_mask(scores, mask)
```

* ƒê√¢y l√† t√≠nh **scaled dot-product attention**
* C√≥ c·ªông th√™m `relative_bias` (t·ª´ pos\_emb) ‚Üí m√¥ h√¨nh h√≥a s·ª± **ch√™nh l·ªách v·ªã tr√≠** theo chi·ªÅu h∆∞·ªõng t∆∞∆°ng ƒë·ªëi.

#### 5. **Truncate context n·∫øu c√≥**

```python
if truncated_context_size > 0:
    # zero out scores beyond left/right limits
```

* Trong hu·∫•n luy·ªán ho·∫∑c simulate streaming, ta c√≥ th·ªÉ gi·ªõi h·∫°n khung attention c·ªßa m·ªói token.

#### 6. **Softmax + weighted sum**

```python
attn_probs = F.softmax(scores, dim=-1)
attn_output = torch.matmul(attn_probs, v)  # (batch, head, time, dim_per_head)
```

#### 7. **Combine heads & output**

```python
output = self.linear_out(attn_output.transpose)  # ‚Üí (batch, time, dim)
```

---

## üß© √ù t∆∞·ªüng t·ªïng th·ªÉ:

### üì¶ N·∫øu b·∫°n coi m·ªói chunk nh∆∞ 1 c·ª≠a s·ªï th·ªùi gian:

* **KV Cache** gi·ªØ c√°c c·ª≠a s·ªï tr∆∞·ªõc ƒë√≥ (ng·ªØ c·∫£nh l·ªãch s·ª≠)
* **Right context** l√† lookahead nh·ªè (cho ph√©p peeking v·ªÅ ph√≠a t∆∞∆°ng lai)
* **Left context** l√† cache c≈© (c√°c chunk tr∆∞·ªõc)
* **Truncation** m√¥ ph·ªèng gi·ªõi h·∫°n streaming (ƒë√†o t·∫°o gi·ªëng inference)

### üö¶ ChunkFormer c·∫£i ti·∫øn h∆°n Conformer ·ªü ch·ªó:

* Bi·∫øt r√µ th·ªùi gian b·∫Øt ƒë·∫ßu/offset c·ªßa t·ª´ng chunk (d√πng offset ƒë·ªÉ d·ªãch attention index ch√≠nh x√°c)
* S·ª≠ d·ª•ng attention window c√≥ th·ªÉ t√πy ch·ªânh
* V·ª´a d√πng **cache + mask + relative pos** ƒë·ªÉ m√¥ h√¨nh h√≥a ng·ªØ c·∫£nh ƒëa chi·ªÅu.

---

## ‚úÖ K·∫øt lu·∫≠n

B·∫°n c√≥ th·ªÉ xem `self_attn.forward_parallel_chunk()` l√† **linh h·ªìn c·ªßa m√¥ h√¨nh streaming attention**:

| Th√†nh ph·∫ßn         | M·ª•c ƒë√≠ch                        |
| ------------------ | ------------------------------- |
| Linear projections | Bi·∫øn ƒë·∫ßu v√†o th√†nh q, k, v      |
| KV cache concat    | Gh√©p n·ªëi context qu√° kh·ª©        |
| Relative bias      | Encode t∆∞∆°ng quan v·ªã tr√≠        |
| Mask + truncate    | Ki·ªÉm so√°t frame n√†o ƒë∆∞·ª£c attend |
| Weighted sum       | T√≠nh attention output           |
| Linear out         | G·ªôp l·∫°i nhi·ªÅu head              |

---

N·∫øu b·∫°n c·∫ßn m√¨nh vi·∫øt l·∫°i ƒëo·∫°n `forward_parallel_chunk` c·ªßa `self_attn` v·ªõi ƒë·∫ßy ƒë·ªß `print()` trace v√† comment nh∆∞ ph·∫ßn tr∆∞·ªõc th√¨ m√¨nh c√≥ th·ªÉ l√†m ngay. B·∫°n mu·ªën kh√¥ng?

