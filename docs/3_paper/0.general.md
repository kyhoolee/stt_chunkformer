# ChunkFormer Training Core Logic

This README summarizes the mathematical theory, data flow, and implementation outline for the core logic of ChunkFormer training. You can copy this file locally for easy reference.

## 1. Streaming Chunk Mechanism

* **Chunk definition**:

  ```
  x_i = x[t_0 + i * H : t_0 + i * H + L]
  ```

  * `L` = chunk length (samples)
  * `H` = hop size (samples)
* **Carry-over state**:

  * Maintain previous chunk's final hidden states as cache for next chunk (inference).
  * Ensures continuity across chunk boundaries.

## 2. Conformer Block

1. **Feed-Forward Module** (Two-layer FFN with Swish activation):

   ```
   FFN(x) = W2 * Swish(W1 * x + b1) + b2
   ```
2. **Multi-Head Self-Attention**:

   * Queries, Keys, Values: $Q, K, V = xW_q, xW_k, xW_v$
   * Attention: $	ext{softmax}(rac{QK^T}{\sqrt{d_k}} + 	ext{mask}) V$
3. **Convolution Module**:

   * Point-wise conv → Depth-wise conv → Point-wise conv
   * Gating (GLU) between conv layers
4. **Residual & LayerNorm**:

   * Pre-norm: LayerNorm applied before sub-layer
   * Residual: Add input to sub-layer output to preserve gradients

> **Shape**: Input `(B, T, D)` → Output `(B, T, D)`

## 3. Positional Encoding

* **Absolute PE**:

  ```
  PE_{(t,2i)}   = sin(t / 10000^{2i/D})
  PE_{(t,2i+1)} = cos(t / 10000^{2i/D})
  ```
* **Relative PE**: learnable biases based on distance between positions

## 4. CTC Head

* **Linear projection**:

  * Hidden states `(B, T, D)` → logits `(B, T, V)`
  * $V$ = vocabulary size
* **CTC Loss**:

  $$
  \mathcal{L}_{CTC} = -\ln \sum_{\pi \in \mathcal{B}^{-1}(y)} \prod_{t=1}^T p(\pi_t \mid x)
  $$

## 5. (Optional) AED Decoder

* **Cross-entropy Loss** with teacher-forcing:

  * Decoder input: previous ground-truth token + context
  * Causal mask to prevent future tokens
* **Shape**: Decoder hidden `(B, U, D)` → logits `(B, U, V)`

## 6. Shape & Data Flow

```text
(waveform)         (features)            (hidden)           (logits)
  (B, L)   → fbank → (B, T, M) → encoder → (B, T, D) → CTC → (B, T, V)
```

* `L`: chunk size in samples
* `T`: number of frames
* `M`: number of Mel bins (e.g., 80)
* `D`: model dimension (e.g., 256)
* `V`: vocabulary size

## 7. Next Steps

1. **Create summary table** of each layer:

   * Purpose, formula, input/output shapes
2. **Instrument code**:

   * Add hooks/prints after each module to verify shapes
3. **Cross-check** theory vs. implementation:

   * Compare formulas with code logic
4. **Discuss unclear points**:

   * Post questions from paper for detailed review

---

*Copy this README into your local project root to guide implementation and review.*
