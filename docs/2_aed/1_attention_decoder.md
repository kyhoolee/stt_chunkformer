Tuyá»‡t! Dá»±a trÃªn cÃ¡c thÃ´ng tin báº¡n cung cáº¥p, mÃ¬nh cÃ³ thá»ƒ **dá»±ng láº¡i logic `TransformerDecoder`** (AED head) tá»« `left_decoder` vá»›i Ä‘áº§y Ä‘á»§ thÃ´ng sá»‘ nhÆ° sau:

---

## âœ… PhÃ¢n tÃ­ch kiáº¿n trÃºc

Tá»« checkpoint:

* `embed.0.weight â†’ (6992, 512)` â†’ **vocab\_size = 6992**, `d_model = 512`
* `feed_forward.w_1.weight â†’ (2048, 512)` â†’ **ffn\_dim = 2048**
* CÃ³ 3 block: `decoders.0`, `decoders.1`, `decoders.2` â†’ **num\_blocks = 3**
* Táº¥t cáº£ attention layers Ä‘á»u cÃ³ `linear_q.weight â†’ (512, 512)` â†’ suy ra: **attention\_heads = 8** (má»—i head 64 dim)

---

## ğŸ§© MÃ´ hÃ¬nh `TransformerDecoder` tÆ°Æ¡ng thÃ­ch vá»›i `left_decoder`

Báº¡n cÃ³ thá»ƒ Ä‘áº·t vÃ o `chunkformer_vpb/modules/transformer_decoder.py`:

```python
import torch
import torch.nn as nn
from wenet.transformer.attention import MultiHeadedAttention
from wenet.transformer.decoder_layer import DecoderLayer
from wenet.transformer.positionwise_feed_forward import PositionwiseFeedForward
from wenet.utils.common import make_pad_mask


class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, encoder_output_size, attention_heads=8,
                 linear_units=2048, num_blocks=3, dropout_rate=0.1):
        super().__init__()

        self.embed = nn.Sequential(
            nn.Embedding(vocab_size, encoder_output_size),
            nn.Dropout(p=dropout_rate)
        )

        self.decoders = nn.ModuleList([
            DecoderLayer(
                size=encoder_output_size,
                self_attn=MultiHeadedAttention(attention_heads, encoder_output_size, dropout_rate),
                src_attn=MultiHeadedAttention(attention_heads, encoder_output_size, dropout_rate),
                feed_forward=PositionwiseFeedForward(
                    encoder_output_size, linear_units, dropout_rate
                ),
                dropout_rate=dropout_rate
            ) for _ in range(num_blocks)
        ])

        self.after_norm = nn.LayerNorm(encoder_output_size)
        self.output_layer = nn.Linear(encoder_output_size, vocab_size)

    def forward(self, tgt, tgt_mask, memory, memory_mask):
        xs = self.embed(tgt)
        for decoder in self.decoders:
            xs, tgt_mask, memory, memory_mask = decoder(xs, tgt_mask, memory, memory_mask)
        xs = self.after_norm(xs)
        return self.output_layer(xs)
```

---

## ğŸ“¥ CÃ¡ch gá»i trong inference

Báº¡n cÃ³ thá»ƒ thÃªm vÃ o `asr_model.py`:

```python
from chunkformer_vpb.modules.transformer_decoder import TransformerDecoder

self.decoder = TransformerDecoder(
    vocab_size=6992,
    encoder_output_size=512,
    attention_heads=8,
    linear_units=2048,
    num_blocks=3,
    dropout_rate=0.1
)
```

---

## ğŸ§ª Äá»ƒ kiá»ƒm tra hoáº¡t Ä‘á»™ng:

Trong `decode.py` hoáº·c script test:

```python
tgt = torch.tensor([[1, 23, 45, 2]])  # [B, T] (with <sos> and <eos>)
tgt_mask = (tgt != 0).unsqueeze(1)  # padding mask
encoder_out = ...  # [B, T, D]
encoder_mask = ...  # [B, 1, T]

logits = model.decoder(tgt, tgt_mask, encoder_out, encoder_mask)
```

---

## ğŸ“Œ Ghi chÃº

* Náº¿u báº¡n muá»‘n dÃ¹ng `attention_rescoring` hoáº·c `beam_search`, mÃ¬nh cÃ³ thá»ƒ viáº¿t thÃªm hÃ m Ä‘Ã³.
* Cáº§n giá»¯ Ä‘Ãºng tÃªn cÃ¡c module (`embed`, `after_norm`, `output_layer`, `decoders.*`) Ä‘á»ƒ `load_state_dict` khÃ´ng bá»‹ mismatch.

---

Báº¡n muá»‘n mÃ¬nh viáº¿t thÃªm Ä‘oáº¡n `attention_rescoring()` inference sample cho `ChunkFormer` vá»›i AED khÃ´ng?
