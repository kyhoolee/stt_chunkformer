import torch
from torchaudio.compliance.kaldi import fbank
from torch.utils.benchmark import Timer
from jiwer import wer
import os
from chunkformer_vpb.model_utils import prepare_input_file, decode_long_form, get_default_args
from chunkformer_vpb.model_utils import init, dump_module_structure, decode_aed_long_form



model_checkpoint = "../../chunkformer-large-vie"  # adjust if needed
output_dir = "model_architect"
os.makedirs(output_dir, exist_ok=True)

# === Load model ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model, char_dict = init(model_checkpoint, device)
model.eval()

args = get_default_args()
args.audio_path = "../../debug_wavs/sample_00.wav"
args.label_text = "nửa vòng trái đất hơn bảy năm"
feats = prepare_input_file(args.audio_path, device)
decoded = decode_long_form(feats, model, char_dict, args, device)

ctc_text = decoded
aed_text_raw, aed_text_clean = decode_aed_long_form(feats, model, char_dict, args, device)

print(f"🆚 So sánh:\n- CTC: {ctc_text}\n- AED: {aed_text_clean}")


print("🟢 Prediction     :", decoded)
if args.label_text:
    print("🔵 Ground Truth   :", args.label_text)
    print("❌ WER            :", wer(args.label_text.lower(), decoded.lower()))



#   --model_checkpoint ../chunkformer-large-vie \
#   --audio_path ../debug_wavs/sample_00.wav \
#   --label_text "nửa vòng trái đất hơn bảy năm"

-------------------

================= 🧩 [Encoder.forward_parallel_chunk] START =================
📥 Input shape: torch.Size([1, 236, 80]), xs_origin_lens: [236]
⚙️ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200
📏 Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7
🔹 Sample 0: original_len=236, padded_len=519, pad_frames=283, n_chunks=1, offset=0

🧱 Total chunked xs shape: torch.Size([1, 519, 80])
📐 xs_lens (post chunk): torch.Size([1]), total_chunks: 1
✅ Applied Global CMVN
🎛️ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])
🧮 att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])
📏 Applied LayerNorm after encoder
📤 Final offset: [28]

✅ [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [28], n_chunks: [1]
====================================================================

📣 AED RAW   : ▁nửa▁vòng▁trái▁đất▁hơn▁bảy▁năm<sos/eos>
📣 AED CLEAN : nửa vòng trái đất hơn bảy năm<sos/eos>
🆚 So sánh:
- CTC:  nửa vòng trái đất hơn bảy năm
- AED: nửa vòng trái đất hơn bảy năm<sos/eos>
🟢 Prediction     :  nửa vòng trái đất hơn bảy năm
🔵 Ground Truth   : nửa vòng trái đất hơn bảy năm
❌ WER            : 0.0

-----------------------------


@torch.no_grad()
def decode_aed_long_form(xs, model, char_dict, args, device):
    chunk_size = args.chunk_size
    left_context_size = args.left_context_size
    right_context_size = args.right_context_size
    subsampling_factor = model.encoder.embed.subsampling_factor
    conv_kernel_size = model.encoder.cnn_module_kernel
    conv_lorder = conv_kernel_size // 2
    num_blocks = model.encoder.num_blocks
    hidden_dim = model.encoder._output_size
    attention_heads = model.encoder.attention_heads

    max_len = int(args.total_batch_duration // 0.01) // 2
    multiply_n = max_len // chunk_size // subsampling_factor
    truncated_context_size = chunk_size * multiply_n
    rel_right_context_size = (
        right_context_size + max(chunk_size, right_context_size) * (num_blocks - 1)
    ) * subsampling_factor

    # Init cache
    offset = torch.zeros(1, dtype=torch.int, device=device)
    att_cache = torch.zeros((num_blocks, left_context_size, attention_heads, hidden_dim * 2 // attention_heads), device=device)
    cnn_cache = torch.zeros((num_blocks, hidden_dim, conv_lorder), device=device)

    encoder_outs_chunks = []
    encoder_lens_total = 0

    for idx in range(0, xs.shape[1], truncated_context_size * subsampling_factor):
        start = truncated_context_size * subsampling_factor * idx
        end = min(start + truncated_context_size * subsampling_factor + 7, xs.shape[1])

        x = xs[:, start:end + rel_right_context_size]
        x_len = torch.tensor([x[0].shape[0]], dtype=torch.int, device=device)

        encoder_outs, encoder_lens, _, att_cache, cnn_cache, offset = model.encoder.forward_parallel_chunk(
            xs=x, 
            xs_origin_lens=x_len,
            chunk_size=chunk_size,
            left_context_size=left_context_size,
            right_context_size=right_context_size,
            att_cache=att_cache,
            cnn_cache=cnn_cache,
            truncated_context_size=truncated_context_size,
            offset=offset
        )

        encoder_outs = encoder_outs[:, :encoder_lens]
        encoder_outs_chunks.append(encoder_outs)
        encoder_lens_total += encoder_outs.shape[1]

        if start + rel_right_context_size >= xs.shape[1]:
            break

    # Cat all chunk outputs
    encoder_outs_full = torch.cat(encoder_outs_chunks, dim=1)
    encoder_mask = torch.ones(1, 1, encoder_outs_full.shape[1], dtype=torch.bool, device=device)

    # Decode AED
    pred_token_ids, _ = model.decode_aed(encoder_outs_full, encoder_mask, maxlen=100)
    pred_ids = pred_token_ids[0].tolist()  # Đổi tên cho ngắn gọn

    # Raw vs Clean
    raw = transcript_raw(pred_ids, char_dict)
    clean = transcript_clean(pred_ids, char_dict)

    print(f"📣 AED RAW   : {raw}")
    print(f"📣 AED CLEAN : {clean}")

    return raw, clean

-------------------------
from wenet.transformer.decoder import BiTransformerDecoder

@torch.no_grad()
def init(model_checkpoint, device):
    config_path = os.path.join(model_checkpoint, "config.yaml")
    checkpoint_path = os.path.join(model_checkpoint, "pytorch_model.bin")
    symbol_table_path = os.path.join(model_checkpoint, "vocab.txt")

    # 1. Load config
    with open(config_path, 'r') as fin:
        config = yaml.load(fin, Loader=yaml.FullLoader)

    # 2. Init model (encoder + ctc)
    model = init_model(config, config_path)
    model.eval()

    # 3. Load full checkpoint
    ckpt = torch.load(checkpoint_path, map_location="cpu")
    load_checkpoint(model, checkpoint_path)

    # 4. Add decoder (AED) if exists in checkpoint
    if any(k.startswith("decoder.left_decoder") for k in ckpt):
        vocab_size = ckpt["decoder.left_decoder.embed.0.weight"].size(0)
        d_model = ckpt["decoder.left_decoder.after_norm.weight"].size(0)
        block_ids = {int(k.split('.')[3]) for k in ckpt if k.startswith("decoder.left_decoder.decoders.")}
        num_blocks = max(block_ids) + 1

        decoder = BiTransformerDecoder(
            vocab_size=vocab_size,
            encoder_output_size=d_model,
            attention_heads=8,
            linear_units=2048,
            num_blocks=num_blocks,
            r_num_blocks=0,
            dropout_rate=0.1,
            positional_dropout_rate=0.1,
            self_attention_dropout_rate=0.0,
            src_attention_dropout_rate=0.0,
            input_layer="embed",
            use_output_layer=True,
            normalize_before=True,
            src_attention=True,
            query_bias=True,
            key_bias=True,
            value_bias=True,
            activation_type="relu",
            gradient_checkpointing=False,
            tie_word_embedding=False,
            use_sdpa=False,
            layer_norm_type='layer_norm',
            norm_eps=1e-5,
            n_kv_head=None,
            head_dim=None,
            mlp_type='position_wise_feed_forward',
            mlp_bias=True,
            n_expert=8,
            n_expert_activated=2
        )

        # ✅ Load đúng phần left_decoder
        decoder.left_decoder.load_state_dict({
            k[len("decoder.left_decoder."):]: v
            for k, v in ckpt.items()
            if k.startswith("decoder.left_decoder.")
        }, strict=False)

        decoder = decoder.to(device)
        decoder.eval()

        model.decoder = decoder  # Attach to main model

    # 5. Move parts to device
    model.encoder = model.encoder.to(device)
    model.ctc = model.ctc.to(device)

    # 6. Build char dictionary
    symbol_table = read_symbol_table(symbol_table_path)
    char_dict = {v: k for k, v in symbol_table.items()}

    return model, char_dict



-------------------------
from collections import defaultdict
from typing import Dict, List, Optional, Tuple

import torch
import logging

from torch.nn.utils.rnn import pad_sequence
from .ctc import CTC
from .utils.common import (IGNORE_ID, add_sos_eos, log_add,
                                remove_duplicates_and_blank, th_accuracy,
                                reverse_pad_list)
# from .mwer import MWER
from .utils.mask import (make_pad_mask, mask_finished_preds,
                              mask_finished_scores, subsequent_mask)


class ASRModel(torch.nn.Module):
    """CTC-attention hybrid Encoder-Decoder model"""
    def __init__(
        self,
        vocab_size: int,
        encoder,
        ctc: CTC,
        ctc_weight: float = 0.5,
        ignore_id: int = IGNORE_ID,
        reverse_weight: float = 0.0,
        decoder=None
    ):
        assert 0.0 <= ctc_weight <= 1.0, ctc_weight

        super().__init__()
        # note that eos is the same as sos (equivalent ID)
        self.sos = vocab_size - 1
        self.eos = vocab_size - 1
        self.vocab_size = vocab_size
        self.ignore_id = ignore_id
        self.ctc_weight = ctc_weight
        self.reverse_weight = reverse_weight

        self.encoder = encoder
        self.encoder.ctc = ctc
        self.ctc = ctc        

        self.decoder = decoder

    def decode_aed(self, encoder_out: torch.Tensor, encoder_mask: torch.Tensor, maxlen: int = 100):
        """
        Greedy AED decoding:  
          - encoder_out: [B, T_in, D]  
          - encoder_mask: [B, 1, T_in]  
        Trả về:  
          - token IDs [B, T_out], score sequence [B, T_out, V]
        """
        batch_size = encoder_out.size(0)
        # Khởi tạo ys với sos
        ys = encoder_out.new_full((batch_size, 1), fill_value=self.sos, dtype=torch.long)
        scores = []

        for _ in range(maxlen):
            # Tạo tensor độ dài của ys
            ys_lens = torch.tensor([ys.size(1)] * batch_size, dtype=torch.long, device=ys.device)
            # Gọi decoder với đúng thứ tự args
            l_x, r_x, olens = self.decoder(
                encoder_out,    # memory
                encoder_mask,   # memory_mask
                ys,             # ys_in_pad
                ys_lens,        # ys_in_lens
                None,           # r_ys_in_pad (không dùng reverse here)
                0.0,            # reverse_weight
            )
            # l_x: [B, T_cur, V]
            # Lấy logits của token cuối cùng
            prob = torch.nn.functional.log_softmax(l_x[:, -1], dim=-1)
            next_token = prob.argmax(dim=-1, keepdim=True)  # [B,1]
            ys = torch.cat([ys, next_token], dim=1)         # [B, T_cur+1]
            scores.append(prob)

            # Nếu tất cả batch đều sinh eos thì dừng sớm
            if (next_token == self.eos).all():
                break

        # Trả về sequence bỏ sos + scores
        return ys[:, 1:], torch.stack(scores, dim=1)


---------------------------

Bạn đã support mình bổ sung phần AED 
- Sử dụng wenet để dùng đúng BiTransformerDecoder
- Bổ sung decode_aed vào ASR model 
- Dùng decode_aed và viết full method để transcript cho 1 audio từ output của AED - kết quả tương đương với CTC 

-> Giờ bạn support mình viết phần Tính Loss đúng theo định nghĩa của chunkformer 
- đúng cách tính 
- import logic từ wenet -> hạn chế viết lại để đảm bảo tương đương 
- dùng đúng config 