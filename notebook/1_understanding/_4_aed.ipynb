{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5313ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 Loaded checkpoint from: ../../chunkformer-large-vie/pytorch_model.bin\n",
      "📦 Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)\n",
      "🔍 AED decoder head included in checkpoint? ✅ YES\n",
      "📊 Model total params: 113,852,240, trainable: 113,852,240\n",
      "✅ Loaded state_dict with:\n",
      "   🔺 Missing keys: 2\n",
      "     - encoder.ctc.ctc_lo.weight\n",
      "     - encoder.ctc.ctc_lo.bias\n",
      "   ⚠️ Unexpected keys in checkpoint: 166\n",
      "     - decoder.left_decoder.embed.0.weight\n",
      "     - decoder.left_decoder.after_norm.weight\n",
      "     - decoder.left_decoder.after_norm.bias\n",
      "     - decoder.left_decoder.output_layer.weight\n",
      "     - decoder.left_decoder.output_layer.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_q.weight\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_q.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_k.weight\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_k.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_v.weight\n",
      "     ...\n",
      "\n",
      "🔍 Keys related to left_decoder:\n",
      "\n",
      "decoder.left_decoder.embed.0.weight → (6992, 512)\n",
      "decoder.left_decoder.after_norm.weight → (512,)\n",
      "decoder.left_decoder.after_norm.bias → (512,)\n",
      "decoder.left_decoder.output_layer.weight → (6992, 512)\n",
      "decoder.left_decoder.output_layer.bias → (6992,)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_q.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_q.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_k.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_k.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_v.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_v.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_out.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.self_attn.linear_out.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_q.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_q.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_k.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_k.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_v.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_v.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_out.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.0.src_attn.linear_out.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.feed_forward.w_1.weight → (2048, 512)\n",
      "decoder.left_decoder.decoders.0.feed_forward.w_1.bias → (2048,)\n",
      "decoder.left_decoder.decoders.0.feed_forward.w_2.weight → (512, 2048)\n",
      "decoder.left_decoder.decoders.0.feed_forward.w_2.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.norm1.weight → (512,)\n",
      "decoder.left_decoder.decoders.0.norm1.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.norm2.weight → (512,)\n",
      "decoder.left_decoder.decoders.0.norm2.bias → (512,)\n",
      "decoder.left_decoder.decoders.0.norm3.weight → (512,)\n",
      "decoder.left_decoder.decoders.0.norm3.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_q.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_q.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_k.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_k.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_v.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_v.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_out.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.self_attn.linear_out.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_q.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_q.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_k.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_k.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_v.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_v.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_out.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.1.src_attn.linear_out.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.feed_forward.w_1.weight → (2048, 512)\n",
      "decoder.left_decoder.decoders.1.feed_forward.w_1.bias → (2048,)\n",
      "decoder.left_decoder.decoders.1.feed_forward.w_2.weight → (512, 2048)\n",
      "decoder.left_decoder.decoders.1.feed_forward.w_2.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.norm1.weight → (512,)\n",
      "decoder.left_decoder.decoders.1.norm1.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.norm2.weight → (512,)\n",
      "decoder.left_decoder.decoders.1.norm2.bias → (512,)\n",
      "decoder.left_decoder.decoders.1.norm3.weight → (512,)\n",
      "decoder.left_decoder.decoders.1.norm3.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_q.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_q.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_k.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_k.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_v.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_v.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_out.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.self_attn.linear_out.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_q.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_q.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_k.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_k.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_v.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_v.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_out.weight → (512, 512)\n",
      "decoder.left_decoder.decoders.2.src_attn.linear_out.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.feed_forward.w_1.weight → (2048, 512)\n",
      "decoder.left_decoder.decoders.2.feed_forward.w_1.bias → (2048,)\n",
      "decoder.left_decoder.decoders.2.feed_forward.w_2.weight → (512, 2048)\n",
      "decoder.left_decoder.decoders.2.feed_forward.w_2.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.norm1.weight → (512,)\n",
      "decoder.left_decoder.decoders.2.norm1.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.norm2.weight → (512,)\n",
      "decoder.left_decoder.decoders.2.norm2.bias → (512,)\n",
      "decoder.left_decoder.decoders.2.norm3.weight → (512,)\n",
      "decoder.left_decoder.decoders.2.norm3.bias → (512,)\n",
      "\n",
      "📦 Suggested groupings:\n",
      "- Embedding: decoder.left_decoder.embed.0.weight\n",
      "- Positional Norm: decoder.left_decoder.after_norm.*\n",
      "- Output head: decoder.left_decoder.output_layer.*\n",
      "- Decoder layers:\n",
      "  - Block 0\n",
      "  - Block 1\n",
      "  - Block 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from chunkformer_vpb.model_utils import init\n",
    "\n",
    "\n",
    "\n",
    "def print_left_decoder_structure(model_checkpoint):\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, char_dict = init(model_checkpoint, device)\n",
    "    model.eval()\n",
    "\n",
    "    # Trích state_dict\n",
    "    state_dict = torch.load(os.path.join(model_checkpoint, \"pytorch_model.bin\"), map_location=device)\n",
    "\n",
    "    print(\"\\n🔍 Keys related to left_decoder:\\n\")\n",
    "    for k in state_dict.keys():\n",
    "        if k.startswith(\"decoder.left_decoder\"):\n",
    "            print(k, \"→\", tuple(state_dict[k].shape))\n",
    "\n",
    "    # Gợi ý nhóm key chính:\n",
    "    print(\"\\n📦 Suggested groupings:\")\n",
    "    print(\"- Embedding: decoder.left_decoder.embed.0.weight\")\n",
    "    print(\"- Positional Norm: decoder.left_decoder.after_norm.*\")\n",
    "    print(\"- Output head: decoder.left_decoder.output_layer.*\")\n",
    "    print(\"- Decoder layers:\")\n",
    "    seen = set()\n",
    "    for k in state_dict.keys():\n",
    "        if k.startswith(\"decoder.left_decoder.decoders.\"):\n",
    "            block = k.split('.')[3]\n",
    "            if block not in seen:\n",
    "                print(f\"  - Block {block}\")\n",
    "                seen.add(block)\n",
    "\n",
    "\n",
    "model_ckpt_path = \"../../chunkformer-large-vie\"  # adjust path\n",
    "print_left_decoder_structure(model_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d25375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (stt310)",
   "language": "python",
   "name": "stt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
