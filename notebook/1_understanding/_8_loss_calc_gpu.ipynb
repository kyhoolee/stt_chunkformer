{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf038ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¾ Loaded checkpoint from: ../../../chunkformer-large-vie/pytorch_model.bin\n",
      "ðŸ“¦ Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)\n",
      "ðŸ” AED decoder head included in checkpoint? âœ… YES\n",
      "ðŸ“Š Model total params: 113,852,240, trainable: 113,852,240\n",
      "\n",
      "================= ðŸ§© [Encoder.forward_parallel_chunk] START =================\n",
      "ðŸ“¥ Input shape: torch.Size([1, 423, 80]), xs_origin_lens: [423]\n",
      "âš™ï¸ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200\n",
      "ðŸ“ Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7\n",
      "\n",
      "ðŸ§± Total chunked xs shape: torch.Size([1, 519, 80])\n",
      "ðŸ“ xs_lens (post chunk): torch.Size([1]), total_chunks: 1\n",
      "ðŸŽ›ï¸ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])\n",
      "ðŸ§® att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])\n",
      "ðŸ§© Layer 0: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 1: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 2: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 3: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 4: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 5: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 6: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 7: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 8: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 9: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 10: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 11: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 12: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 13: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 14: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 15: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ§© Layer 16: xs shape after layer = torch.Size([1, 64, 512])\n",
      "ðŸ“ Applied LayerNorm after encoder\n",
      "ðŸ“¤ Final offset: [52]\n",
      "\n",
      "âœ… [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [52], n_chunks: [1]\n",
      "====================================================================\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>  ys_out shape: torch.Size([1, 14]), logp shape: torch.Size([1, 14, 6992])\n",
      "Loss: 1.5968455076217651\n",
      "CTC Loss: 3.889507293701172\n",
      "AED Loss: 0.6142760515213013\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from chunkformer_vpb.training.finetune_utils import (\n",
    "    get_default_args,\n",
    "    prepare_input_file,\n",
    "    load_model_only,\n",
    "    GreedyTokenizer,\n",
    "    compute_chunkformer_loss,\n",
    ")\n",
    "\n",
    "def main():\n",
    "    # 1. Chuáº©n bá»‹ args vÃ  device\n",
    "    args = get_default_args()\n",
    "    args.model_checkpoint = \"../../../chunkformer-large-vie\"     # Ä‘Æ°á»ng dáº«n Ä‘áº¿n folder checkpoint\n",
    "    # args.audio_path       = \"../../../debug_wavs/sample_19.wav\"  # file audio máº«u\n",
    "    # args.label_text       = \"cáº§n náº¯m báº¯t xu hÆ°á»›ng phÃ¡t triá»ƒn cÃ´ng nghá»‡ má»›i\"  # label ground-truth\n",
    "    args.audio_path       = \"../../../debug_wavs/utt_000664.wav\"  # file audio máº«u\n",
    "    args.label_text       = \"má»™t giá»ng nÃ³i du dÆ°Æ¡ng khÃ´ng thá»ƒ láº«n vá»›i ai khÃ¡c cáº¥t lÃªn\"  # label ground-truth\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 2. Load model + tokenizer\n",
    "    model, _ = load_model_only(args.model_checkpoint, device)\n",
    "    model.ctc_weight = 0.3\n",
    "    # model.reverse_weight = 0.3 -> can not work due to there is no right_decoder \n",
    "    tokenizer = GreedyTokenizer(vocab_path=f\"{args.model_checkpoint}/vocab.txt\")\n",
    "\n",
    "    # 3. Prepare input features\n",
    "    xs = prepare_input_file(args.audio_path, device)  # [1, T_raw, 80]\n",
    "\n",
    "    # 4. Compute loss\n",
    "    loss_dict = compute_chunkformer_loss(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        xs=xs,\n",
    "        args=args,\n",
    "        label_text=args.label_text,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5. In káº¿t quáº£\n",
    "    print(f\"Loss: {loss_dict['loss'].item()}\")\n",
    "    print(f\"CTC Loss: {loss_dict['loss_ctc'].item()}\")\n",
    "    print(f\"AED Loss: {loss_dict['loss_att'].item()}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55473ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b88f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stt310)",
   "language": "python",
   "name": "stt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
