{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef09867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character  found_in_vocab\n",
      "        a            True\n",
      "        ă            True\n",
      "        â            True\n",
      "        b            True\n",
      "        c            True\n",
      "        d            True\n",
      "        đ            True\n",
      "        e            True\n",
      "        ê            True\n",
      "        g            True\n",
      "        h            True\n",
      "        i            True\n",
      "        k            True\n",
      "        l            True\n",
      "        m            True\n",
      "        n            True\n",
      "        o            True\n",
      "        ô            True\n",
      "        ơ            True\n",
      "        p            True\n",
      "        q            True\n",
      "        r            True\n",
      "        s            True\n",
      "        t            True\n",
      "        u            True\n",
      "        ư            True\n",
      "        v            True\n",
      "        y            True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Đường dẫn tới file vocab.txt (chỉnh lại cho đúng bên bạn)\n",
    "vocab_path = \"../../chunkformer-large-vie/vocab.txt\"\n",
    "\n",
    "if not os.path.exists(vocab_path):\n",
    "    raise FileNotFoundError(f\"Không tìm thấy file vocab.txt tại {vocab_path}. Vui lòng kiểm tra lại đường dẫn.\")\n",
    "\n",
    "# 2. Danh sách 29 ký tự cơ bản tiếng Việt\n",
    "vietnamese_chars = list(\"aăâbcdđeêghiklmnoôơpqrstuưvy\")\n",
    "\n",
    "# 3. Load toàn bộ token từ vocab.txt\n",
    "tokens = []\n",
    "with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        token = line.strip().split()[0]\n",
    "        tokens.append(token)\n",
    "\n",
    "# 4. Kiểm tra sự xuất hiện của mỗi ký tự\n",
    "results = []\n",
    "for char in vietnamese_chars:\n",
    "    # Một token có thể là ký tự đơn (e.g. 'a') hoặc subword (e.g. '▁a')\n",
    "    found = any(tok == char or tok == f\"▁{char}\" for tok in tokens)\n",
    "    results.append({\"character\": char, \"found_in_vocab\": found})\n",
    "\n",
    "# 5. Hiển thị kết quả\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47adb732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:       hôm nay trời trói trang quá\n",
      "Token IDs:  [2967, 4136, 5954, 5876, 5818, 4918]\n",
      "Tokens:     ['▁hôm', '▁nay', '▁trời', '▁trói', '▁trang', '▁quá']\n",
      "2967: ▁hôm\n",
      "4136: ▁nay\n",
      "5954: ▁trời\n",
      "5876: ▁trói\n",
      "5818: ▁trang\n",
      "4918: ▁quá\n"
     ]
    }
   ],
   "source": [
    "# 1. Load vocab.txt\n",
    "vocab_path = \"../../chunkformer-large-vie/vocab.txt\"  # chỉnh lại đường dẫn nếu cần\n",
    "vocab = []\n",
    "with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        token = line.strip().split()[0]\n",
    "        vocab.append(token)\n",
    "\n",
    "# 2. Build mapping token -> ID and sorted vocab for greedy\n",
    "token2id = {token: idx for idx, token in enumerate(vocab)}\n",
    "vocab_sorted = sorted(vocab, key=len, reverse=True)\n",
    "\n",
    "# 3. Greedy longest-match tokenization function\n",
    "def greedy_tokenize(text: str):\n",
    "    s = \"▁\" + text.strip().replace(\" \", \"▁\")\n",
    "    idx = 0\n",
    "    L = len(s)\n",
    "    ids = []\n",
    "    while idx < L:\n",
    "        for token in vocab_sorted:\n",
    "            if s.startswith(token, idx):\n",
    "                ids.append(token2id[token])\n",
    "                idx += len(token)\n",
    "                break\n",
    "        else:\n",
    "            unk_id = token2id.get(\"<unk>\")\n",
    "            if unk_id is not None:\n",
    "                ids.append(unk_id)\n",
    "            idx += 1\n",
    "    return ids\n",
    "\n",
    "# 4. Example usage\n",
    "sample_text = \"hôm nay trời trói trang quá\"\n",
    "# \"hôm nay trời chói chang quá\"\n",
    "# \"hum nay trời chói chang quớ\"\n",
    "# \"nửa vòng trái đất hơn bảy năm\"\n",
    "token_ids = greedy_tokenize(sample_text)\n",
    "\n",
    "# 5. Map back IDs -> tokens and print\n",
    "tokens = [vocab[i] for i in token_ids]\n",
    "print(\"Text:      \", sample_text)\n",
    "print(\"Token IDs: \", token_ids)\n",
    "print(\"Tokens:    \", tokens)\n",
    "\n",
    "# Or, print one per line:\n",
    "for idx, tok in zip(token_ids, tokens):\n",
    "    print(f\"{idx}: {tok}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04dd513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:      xử lý dữ liệu\n",
      "Token IDs: [6591, 3636, 2433, 3517]\n",
      "Tokens:    ['▁xử', '▁lý', '▁dữ', '▁liệu']\n",
      "\n",
      "Text:      quốc hội\n",
      "Token IDs: [4966, 3059]\n",
      "Tokens:    ['▁quốc', '▁hội']\n",
      "\n",
      "Text:      self-service\n",
      "Token IDs: [5216, 1, 740, 854, 159]\n",
      "Tokens:    ['▁self', '<unk>', 'ser', 'vi', 'ce']\n",
      "\n",
      "Text:      đặc biệt\n",
      "Token IDs: [6844, 1484]\n",
      "Tokens:    ['▁đặc', '▁biệt']\n",
      "\n",
      "Text:      học máy nâng cao\n",
      "Token IDs: [3037, 3958, 4541, 1755]\n",
      "Tokens:    ['▁học', '▁máy', '▁nâng', '▁cao']\n"
     ]
    }
   ],
   "source": [
    "# 1. Load vocab.txt\n",
    "vocab_path = \"../../chunkformer-large-vie/vocab.txt\"\n",
    "vocab = []\n",
    "with open(vocab_path, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        token = line.strip().split()[0]\n",
    "        vocab.append(token)\n",
    "\n",
    "# 2. Build mapping and sorted vocab\n",
    "token2id = {token: idx for idx, token in enumerate(vocab)}\n",
    "vocab_sorted = sorted(vocab, key=len, reverse=True)\n",
    "\n",
    "# 3. Greedy longest-match tokenizer\n",
    "def greedy_tokenize(text: str):\n",
    "    s = \"▁\" + text.strip().replace(\" \", \"▁\")\n",
    "    idx = 0\n",
    "    L = len(s)\n",
    "    ids = []\n",
    "    while idx < L:\n",
    "        for token in vocab_sorted:\n",
    "            if s.startswith(token, idx):\n",
    "                ids.append(token2id[token])\n",
    "                idx += len(token)\n",
    "                break\n",
    "        else:\n",
    "            # fallback: map char to itself if in vocab, else <unk>\n",
    "            ch = s[idx]\n",
    "            if ch in token2id:\n",
    "                ids.append(token2id[ch])\n",
    "            else:\n",
    "                unk_id = token2id.get(\"<unk>\")\n",
    "                if unk_id is not None:\n",
    "                    ids.append(unk_id)\n",
    "            idx += 1\n",
    "    return ids\n",
    "\n",
    "# 4. Test cases\n",
    "test_texts = [\n",
    "    \"xử lý dữ liệu\",\n",
    "    \"quốc hội\",\n",
    "    \"self-service\",\n",
    "    \"đặc biệt\",\n",
    "    \"học máy nâng cao\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    ids = greedy_tokenize(text)\n",
    "    toks = [vocab[i] for i in ids]\n",
    "    print(f\"\\nText:      {text}\")\n",
    "    print(f\"Token IDs: {ids}\")\n",
    "    print(f\"Tokens:    {toks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48657d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c337b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b13b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b56886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (stt310)",
   "language": "python",
   "name": "stt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
