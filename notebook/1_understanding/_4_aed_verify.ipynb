{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f6932b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== üß™ Decoder Checkpoint Verification ===\n",
      "üîπ Total model params: 84\n",
      "‚úÖ Matched params     : 83\n",
      "‚ùå Missing params     : 1\n",
      "‚ùå Mismatched shapes  : 0\n",
      "‚ö†Ô∏è Extra params in ckpt: 0\n",
      "\n",
      "‚úÖ Matching Keys:\n",
      "   + embed.0.weight\n",
      "   + after_norm.weight\n",
      "   + after_norm.bias\n",
      "   + output_layer.weight\n",
      "   + output_layer.bias\n",
      "   + decoders.0.self_attn.linear_q.weight\n",
      "   + decoders.0.self_attn.linear_q.bias\n",
      "   + decoders.0.self_attn.linear_k.weight\n",
      "   + decoders.0.self_attn.linear_k.bias\n",
      "   + decoders.0.self_attn.linear_v.weight\n",
      "   + decoders.0.self_attn.linear_v.bias\n",
      "   + decoders.0.self_attn.linear_out.weight\n",
      "   + decoders.0.self_attn.linear_out.bias\n",
      "   + decoders.0.src_attn.linear_q.weight\n",
      "   + decoders.0.src_attn.linear_q.bias\n",
      "   + decoders.0.src_attn.linear_k.weight\n",
      "   + decoders.0.src_attn.linear_k.bias\n",
      "   + decoders.0.src_attn.linear_v.weight\n",
      "   + decoders.0.src_attn.linear_v.bias\n",
      "   + decoders.0.src_attn.linear_out.weight\n",
      "   + decoders.0.src_attn.linear_out.bias\n",
      "   + decoders.0.feed_forward.w_1.weight\n",
      "   + decoders.0.feed_forward.w_1.bias\n",
      "   + decoders.0.feed_forward.w_2.weight\n",
      "   + decoders.0.feed_forward.w_2.bias\n",
      "   + decoders.0.norm1.weight\n",
      "   + decoders.0.norm1.bias\n",
      "   + decoders.0.norm2.weight\n",
      "   + decoders.0.norm2.bias\n",
      "   + decoders.0.norm3.weight\n",
      "   + decoders.0.norm3.bias\n",
      "   + decoders.1.self_attn.linear_q.weight\n",
      "   + decoders.1.self_attn.linear_q.bias\n",
      "   + decoders.1.self_attn.linear_k.weight\n",
      "   + decoders.1.self_attn.linear_k.bias\n",
      "   + decoders.1.self_attn.linear_v.weight\n",
      "   + decoders.1.self_attn.linear_v.bias\n",
      "   + decoders.1.self_attn.linear_out.weight\n",
      "   + decoders.1.self_attn.linear_out.bias\n",
      "   + decoders.1.src_attn.linear_q.weight\n",
      "   + decoders.1.src_attn.linear_q.bias\n",
      "   + decoders.1.src_attn.linear_k.weight\n",
      "   + decoders.1.src_attn.linear_k.bias\n",
      "   + decoders.1.src_attn.linear_v.weight\n",
      "   + decoders.1.src_attn.linear_v.bias\n",
      "   + decoders.1.src_attn.linear_out.weight\n",
      "   + decoders.1.src_attn.linear_out.bias\n",
      "   + decoders.1.feed_forward.w_1.weight\n",
      "   + decoders.1.feed_forward.w_1.bias\n",
      "   + decoders.1.feed_forward.w_2.weight\n",
      "   + decoders.1.feed_forward.w_2.bias\n",
      "   + decoders.1.norm1.weight\n",
      "   + decoders.1.norm1.bias\n",
      "   + decoders.1.norm2.weight\n",
      "   + decoders.1.norm2.bias\n",
      "   + decoders.1.norm3.weight\n",
      "   + decoders.1.norm3.bias\n",
      "   + decoders.2.self_attn.linear_q.weight\n",
      "   + decoders.2.self_attn.linear_q.bias\n",
      "   + decoders.2.self_attn.linear_k.weight\n",
      "   + decoders.2.self_attn.linear_k.bias\n",
      "   + decoders.2.self_attn.linear_v.weight\n",
      "   + decoders.2.self_attn.linear_v.bias\n",
      "   + decoders.2.self_attn.linear_out.weight\n",
      "   + decoders.2.self_attn.linear_out.bias\n",
      "   + decoders.2.src_attn.linear_q.weight\n",
      "   + decoders.2.src_attn.linear_q.bias\n",
      "   + decoders.2.src_attn.linear_k.weight\n",
      "   + decoders.2.src_attn.linear_k.bias\n",
      "   + decoders.2.src_attn.linear_v.weight\n",
      "   + decoders.2.src_attn.linear_v.bias\n",
      "   + decoders.2.src_attn.linear_out.weight\n",
      "   + decoders.2.src_attn.linear_out.bias\n",
      "   + decoders.2.feed_forward.w_1.weight\n",
      "   + decoders.2.feed_forward.w_1.bias\n",
      "   + decoders.2.feed_forward.w_2.weight\n",
      "   + decoders.2.feed_forward.w_2.bias\n",
      "   + decoders.2.norm1.weight\n",
      "   + decoders.2.norm1.bias\n",
      "   + decoders.2.norm2.weight\n",
      "   + decoders.2.norm2.bias\n",
      "   + decoders.2.norm3.weight\n",
      "   + decoders.2.norm3.bias\n",
      "\n",
      "‚ùå Missing in checkpoint:\n",
      "   - embed.1.pe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from wenet.transformer.decoder import BiTransformerDecoder\n",
    "\n",
    "def instantiate_and_compare_verbose(checkpoint_path):\n",
    "    ckpt = torch.load(f\"{checkpoint_path}/pytorch_model.bin\", map_location=\"cpu\")\n",
    "\n",
    "    # === Infer hyperparameters ===\n",
    "    vocab_size = ckpt[\"decoder.left_decoder.embed.0.weight\"].size(0)\n",
    "    d_model = ckpt[\"decoder.left_decoder.after_norm.weight\"].size(0)\n",
    "    block_ids = {int(k.split('.')[3]) for k in ckpt if k.startswith(\"decoder.left_decoder.decoders.\")}\n",
    "    num_blocks = max(block_ids) + 1\n",
    "\n",
    "    decoder = BiTransformerDecoder(\n",
    "        vocab_size=vocab_size,\n",
    "        encoder_output_size=d_model,\n",
    "        attention_heads=8,\n",
    "        linear_units=2048,\n",
    "        num_blocks=num_blocks,\n",
    "        r_num_blocks=0,\n",
    "        dropout_rate=0.1,\n",
    "        positional_dropout_rate=0.1,\n",
    "        self_attention_dropout_rate=0.0,\n",
    "        src_attention_dropout_rate=0.0,\n",
    "        input_layer=\"embed\",\n",
    "        use_output_layer=True,\n",
    "        normalize_before=True,\n",
    "        src_attention=True,\n",
    "        query_bias=True,\n",
    "        key_bias=True,\n",
    "        value_bias=True,\n",
    "        activation_type=\"relu\",\n",
    "        gradient_checkpointing=False,\n",
    "        tie_word_embedding=False,\n",
    "        use_sdpa=False,\n",
    "        layer_norm_type='layer_norm',\n",
    "        norm_eps=1e-5,\n",
    "        n_kv_head=None,\n",
    "        head_dim=None,\n",
    "        mlp_type='position_wise_feed_forward',\n",
    "        mlp_bias=True,\n",
    "        n_expert=8,\n",
    "        n_expert_activated=2\n",
    "    )\n",
    "\n",
    "    model_sd = decoder.left_decoder.state_dict()\n",
    "    ckpt_sd = {\n",
    "        k[len(\"decoder.left_decoder.\"):]: v\n",
    "        for k, v in ckpt.items()\n",
    "        if k.startswith(\"decoder.left_decoder.\")\n",
    "    }\n",
    "\n",
    "    missing, mismatched, extra, matched = [], [], [], []\n",
    "\n",
    "    for name, param in model_sd.items():\n",
    "        if name not in ckpt_sd:\n",
    "            missing.append(name)\n",
    "        elif tuple(param.shape) != tuple(ckpt_sd[name].shape):\n",
    "            mismatched.append((name, tuple(param.shape), tuple(ckpt_sd[name].shape)))\n",
    "        else:\n",
    "            matched.append(name)\n",
    "\n",
    "    extra = sorted(set(ckpt_sd.keys()) - set(model_sd.keys()))\n",
    "\n",
    "    # === Print diagnostics ===\n",
    "    print(\"=== üß™ Decoder Checkpoint Verification ===\")\n",
    "    print(f\"üîπ Total model params: {len(model_sd)}\")\n",
    "    print(f\"‚úÖ Matched params     : {len(matched)}\")\n",
    "    print(f\"‚ùå Missing params     : {len(missing)}\")\n",
    "    print(f\"‚ùå Mismatched shapes  : {len(mismatched)}\")\n",
    "    print(f\"‚ö†Ô∏è Extra params in ckpt: {len(extra)}\")\n",
    "    print(\"\\n‚úÖ Matching Keys:\")\n",
    "    for name in matched:\n",
    "        print(f\"   + {name}\")\n",
    "\n",
    "    if missing:\n",
    "        print(\"\\n‚ùå Missing in checkpoint:\")\n",
    "        for name in missing:\n",
    "            print(f\"   - {name}\")\n",
    "\n",
    "    if mismatched:\n",
    "        print(\"\\n‚ùå Shape mismatches:\")\n",
    "        for name, shape_model, shape_ckpt in mismatched:\n",
    "            print(f\"   - {name}: model={shape_model}, ckpt={shape_ckpt}\")\n",
    "\n",
    "    if extra:\n",
    "        print(\"\\n‚ö†Ô∏è Extra keys in checkpoint:\")\n",
    "        for name in extra:\n",
    "            print(f\"   - {name}\")\n",
    "\n",
    "# Usage\n",
    "path = \"../../chunkformer-large-vie\"\n",
    "instantiate_and_compare_verbose(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73000108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (stt310)",
   "language": "python",
   "name": "stt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
