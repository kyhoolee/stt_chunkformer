{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8277283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module \"torch_npu\" not found. \"pip install torch_npu\"                 if you are using Ascend NPU, otherwise, ignore it\n",
      "\n",
      "ğŸ§¾ Loaded checkpoint from: ../../chunkformer-large-vie/pytorch_model.bin\n",
      "ğŸ“¦ Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)\n",
      "ğŸ” AED decoder head included in checkpoint? âœ… YES\n",
      "ğŸ“Š Model total params: 113,852,240, trainable: 113,852,240\n",
      "âœ… Loaded state_dict with:\n",
      "   ğŸ”º Missing keys: 2\n",
      "     - encoder.ctc.ctc_lo.weight\n",
      "     - encoder.ctc.ctc_lo.bias\n",
      "   âš ï¸ Unexpected keys in checkpoint: 166\n",
      "     - decoder.left_decoder.embed.0.weight\n",
      "     - decoder.left_decoder.after_norm.weight\n",
      "     - decoder.left_decoder.after_norm.bias\n",
      "     - decoder.left_decoder.output_layer.weight\n",
      "     - decoder.left_decoder.output_layer.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_q.weight\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_q.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_k.weight\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_k.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_v.weight\n",
      "     ...\n",
      "\n",
      "ğŸ“¥ Loading file: ../../debug_wavs/sample_00.wav\n",
      "ğŸ” [pydub] Raw frame_rate   : 16000\n",
      "ğŸ” [pydub] Sample width     : 4 bytes (32 bits)\n",
      "ğŸ” [pydub] Channels         : 1\n",
      "ğŸ” [pydub] Duration (ms)    : 2375 ms\n",
      "ğŸ§ª [pydub] Type of array     : <class 'array.array'>, dtype: int16\n",
      "ğŸ§ª [pydub] First 10 samples  : array('h', [0, 0, -1, 0, 0, 0, 1, 3, 9, 11])\n",
      "âœ… [pydub] Waveform shape    : torch.Size([1, 38000])\n",
      "ğŸ“Š [pydub] Min: -22944.00, Max: 24431.00, Mean: -0.01\n",
      "\n",
      "ğŸ” [Compare] Loading with torchaudio.load()\n",
      "âœ… [torchaudio] shape        : torch.Size([1, 38000]), sample_rate: 16000\n",
      "ğŸ“Š [torchaudio] Min: -0.7002, Max: 0.7456, Mean: -0.0000\n",
      "ğŸ“ Diff (mean abs): 0.0000 (assuming torchaudio gives normalized)\n",
      "\n",
      "================= ğŸ§© [Encoder.forward_parallel_chunk] START =================\n",
      "ğŸ“¥ Input shape: torch.Size([1, 236, 80]), xs_origin_lens: [236]\n",
      "âš™ï¸ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200\n",
      "ğŸ“ Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7\n",
      "ğŸ”¹ Sample 0: original_len=236, padded_len=519, pad_frames=283, n_chunks=1, offset=0\n",
      "\n",
      "ğŸ§± Total chunked xs shape: torch.Size([1, 519, 80])\n",
      "ğŸ“ xs_lens (post chunk): torch.Size([1]), total_chunks: 1\n",
      "âœ… Applied Global CMVN\n",
      "ğŸ›ï¸ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])\n",
      "ğŸ§® att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])\n",
      "ğŸ“ Applied LayerNorm after encoder\n",
      "ğŸ“¤ Final offset: [28]\n",
      "\n",
      "âœ… [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [28], n_chunks: [1]\n",
      "====================================================================\n",
      "\n",
      "\n",
      "ğŸ“Š Total full_framewise_ids shape: torch.Size([28])\n",
      "    Sample token IDs (first 20): [0, 0, 0, 0, 4657, 0, 0, 6324, 0, 0, 5854, 0, 0, 6819, 0, 2983, 0, 0, 1635, 0]\n",
      "\n",
      "ğŸ“ Decoded segments (first 3):\n",
      "  â†’ {'decode': ' ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm', 'start': '00:00:00:000', 'end': '00:00:02:160'}\n",
      "\n",
      "âœ… Final transcript:  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "\n",
      "================= ğŸ§© [Encoder.forward_parallel_chunk] START =================\n",
      "ğŸ“¥ Input shape: torch.Size([1, 236, 80]), xs_origin_lens: [236]\n",
      "âš™ï¸ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200\n",
      "ğŸ“ Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7\n",
      "ğŸ”¹ Sample 0: original_len=236, padded_len=519, pad_frames=283, n_chunks=1, offset=0\n",
      "\n",
      "ğŸ§± Total chunked xs shape: torch.Size([1, 519, 80])\n",
      "ğŸ“ xs_lens (post chunk): torch.Size([1]), total_chunks: 1\n",
      "âœ… Applied Global CMVN\n",
      "ğŸ›ï¸ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])\n",
      "ğŸ§® att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])\n",
      "ğŸ“ Applied LayerNorm after encoder\n",
      "ğŸ“¤ Final offset: [28]\n",
      "\n",
      "âœ… [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [28], n_chunks: [1]\n",
      "====================================================================\n",
      "\n",
      "ğŸ“£ AED RAW   : â–ná»­aâ–vÃ²ngâ–trÃ¡iâ–Ä‘áº¥tâ–hÆ¡nâ–báº£yâ–nÄƒm<sos/eos>\n",
      "ğŸ“£ AED CLEAN : ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm<sos/eos>\n",
      "ğŸ†š So sÃ¡nh:\n",
      "- CTC:  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "- AED: ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm<sos/eos>\n",
      "ğŸŸ¢ Prediction     :  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "ğŸ”µ Ground Truth   : ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "âŒ WER            : 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchaudio.compliance.kaldi import fbank\n",
    "from torch.utils.benchmark import Timer\n",
    "from jiwer import wer\n",
    "import os\n",
    "from chunkformer_vpb.model_utils import prepare_input_file, decode_long_form, get_default_args\n",
    "from chunkformer_vpb.model_utils import init, dump_module_structure, decode_aed_long_form\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = \"../../chunkformer-large-vie\"  # adjust if needed\n",
    "output_dir = \"model_architect\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, char_dict = init(model_checkpoint, device)\n",
    "model.eval()\n",
    "\n",
    "args = get_default_args()\n",
    "args.audio_path = \"../../debug_wavs/sample_00.wav\"\n",
    "args.label_text = \"ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\"\n",
    "feats = prepare_input_file(args.audio_path, device)\n",
    "decoded = decode_long_form(feats, model, char_dict, args, device)\n",
    "\n",
    "ctc_text = decoded\n",
    "aed_text_raw, aed_text_clean = decode_aed_long_form(feats, model, char_dict, args, device)\n",
    "\n",
    "print(f\"ğŸ†š So sÃ¡nh:\\n- CTC: {ctc_text}\\n- AED: {aed_text_clean}\")\n",
    "\n",
    "\n",
    "print(\"ğŸŸ¢ Prediction     :\", decoded)\n",
    "if args.label_text:\n",
    "    print(\"ğŸ”µ Ground Truth   :\", args.label_text)\n",
    "    print(\"âŒ WER            :\", wer(args.label_text.lower(), decoded.lower()))\n",
    "\n",
    "\n",
    "\n",
    "#   --model_checkpoint ../chunkformer-large-vie \\\n",
    "#   --audio_path ../debug_wavs/sample_00.wav \\\n",
    "#   --label_text \"ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf038ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module \"torch_npu\" not found. \"pip install torch_npu\"                 if you are using Ascend NPU, otherwise, ignore it\n",
      "\n",
      "ğŸ§¾ Loaded checkpoint from: ../../chunkformer-large-vie/pytorch_model.bin\n",
      "ğŸ“¦ Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)\n",
      "ğŸ” AED decoder head included in checkpoint? âœ… YES\n",
      "ğŸ“Š Model total params: 113,852,240, trainable: 113,852,240\n",
      "âœ… Loaded state_dict with:\n",
      "   ğŸ”º Missing keys: 2\n",
      "     - encoder.ctc.ctc_lo.weight\n",
      "     - encoder.ctc.ctc_lo.bias\n",
      "   âš ï¸ Unexpected keys in checkpoint: 166\n",
      "     - decoder.left_decoder.embed.0.weight\n",
      "     - decoder.left_decoder.after_norm.weight\n",
      "     - decoder.left_decoder.after_norm.bias\n",
      "     - decoder.left_decoder.output_layer.weight\n",
      "     - decoder.left_decoder.output_layer.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_q.weight\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_q.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_k.weight\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_k.bias\n",
      "     - decoder.left_decoder.decoders.0.self_attn.linear_v.weight\n",
      "     ...\n",
      "\n",
      "ğŸ“¥ Loading file: ../../debug_wavs/sample_00.wav\n",
      "ğŸ” [pydub] Raw frame_rate   : 16000\n",
      "ğŸ” [pydub] Sample width     : 4 bytes (32 bits)\n",
      "ğŸ” [pydub] Channels         : 1\n",
      "ğŸ” [pydub] Duration (ms)    : 2375 ms\n",
      "ğŸ§ª [pydub] Type of array     : <class 'array.array'>, dtype: int16\n",
      "ğŸ§ª [pydub] First 10 samples  : array('h', [0, 0, -1, 0, 0, 0, 1, 3, 9, 11])\n",
      "âœ… [pydub] Waveform shape    : torch.Size([1, 38000])\n",
      "ğŸ“Š [pydub] Min: -22944.00, Max: 24431.00, Mean: -0.01\n",
      "\n",
      "ğŸ” [Compare] Loading with torchaudio.load()\n",
      "âœ… [torchaudio] shape        : torch.Size([1, 38000]), sample_rate: 16000\n",
      "ğŸ“Š [torchaudio] Min: -0.7002, Max: 0.7456, Mean: -0.0000\n",
      "ğŸ“ Diff (mean abs): 0.0000 (assuming torchaudio gives normalized)\n",
      "\n",
      "================= ğŸ§© [Encoder.forward_parallel_chunk] START =================\n",
      "ğŸ“¥ Input shape: torch.Size([1, 236, 80]), xs_origin_lens: [236]\n",
      "âš™ï¸ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200\n",
      "ğŸ“ Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7\n",
      "ğŸ”¹ Sample 0: original_len=236, padded_len=519, pad_frames=283, n_chunks=1, offset=0\n",
      "\n",
      "ğŸ§± Total chunked xs shape: torch.Size([1, 519, 80])\n",
      "ğŸ“ xs_lens (post chunk): torch.Size([1]), total_chunks: 1\n",
      "âœ… Applied Global CMVN\n",
      "ğŸ›ï¸ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])\n",
      "ğŸ§® att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])\n",
      "ğŸ“ Applied LayerNorm after encoder\n",
      "ğŸ“¤ Final offset: [28]\n",
      "\n",
      "âœ… [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [28], n_chunks: [1]\n",
      "====================================================================\n",
      "\n",
      "Total Loss: 2.1481\n",
      "  CTC Loss: 6.5648\n",
      "  AED Loss: 0.2553\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from chunkformer_vpb.training.finetune_utils import (\n",
    "    get_default_args,\n",
    "    prepare_input_file,\n",
    "    load_model_only,\n",
    "    GreedyTokenizer,\n",
    "    compute_chunkformer_loss,\n",
    ")\n",
    "\n",
    "def main():\n",
    "    # 1. Chuáº©n bá»‹ args vÃ  device\n",
    "    args = get_default_args()\n",
    "    args.model_checkpoint = \"../../chunkformer-large-vie\"     # Ä‘Æ°á»ng dáº«n Ä‘áº¿n folder checkpoint\n",
    "    args.audio_path       = \"../../debug_wavs/sample_00.wav\"  # file audio máº«u\n",
    "    args.label_text       = \"ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\"  # label ground-truth\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 2. Load model + tokenizer\n",
    "    model, _ = load_model_only(args.model_checkpoint, device)\n",
    "    model.ctc_weight = 0.3\n",
    "    # model.reverse_weight = 0.3 -> can not work due to there is no right_decoder \n",
    "    tokenizer = GreedyTokenizer(vocab_path=f\"{args.model_checkpoint}/vocab.txt\")\n",
    "\n",
    "    # 3. Prepare input features\n",
    "    xs = prepare_input_file(args.audio_path, device)  # [1, T_raw, 80]\n",
    "\n",
    "    # 4. Compute loss\n",
    "    loss_dict = compute_chunkformer_loss(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        xs=xs,\n",
    "        args=args,\n",
    "        label_text=args.label_text,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5. In káº¿t quáº£\n",
    "    print(f\"Total Loss: {loss_dict['loss'].item():.4f}\")\n",
    "    print(f\"  CTC Loss: {loss_dict['loss_ctc'].item():.4f}\")\n",
    "    print(f\"  AED Loss: {loss_dict['loss_att'].item():.4f}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55473ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b88f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (stt310)",
   "language": "python",
   "name": "stt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
