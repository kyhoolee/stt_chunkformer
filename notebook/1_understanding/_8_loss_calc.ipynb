{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8277283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¾ Loaded checkpoint from: ../../../chunkformer-large-vie/pytorch_model.bin\n",
      "ğŸ“¦ Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)\n",
      "ğŸ” AED decoder head included in checkpoint? âœ… YES\n",
      "ğŸ“Š Model total params: 113,852,240, trainable: 113,852,240\n",
      "!!!Vocab size: 6992\n",
      "\n",
      "ğŸ“¥ Loading file: ../../../debug_wavs/sample_00.wav\n",
      "ğŸ” [pydub] Raw frame_rate   : 16000\n",
      "ğŸ” [pydub] Sample width     : 4 bytes (32 bits)\n",
      "ğŸ” [pydub] Channels         : 1\n",
      "ğŸ” [pydub] Duration (ms)    : 2375 ms\n",
      "ğŸ§ª [pydub] Type of array     : <class 'array.array'>, dtype: int16\n",
      "ğŸ§ª [pydub] First 10 samples  : array('h', [0, 0, -1, 0, 0, 0, 1, 3, 9, 11])\n",
      "âœ… [pydub] Waveform shape    : torch.Size([1, 38000])\n",
      "ğŸ“Š [pydub] Min: -22944.00000000, Max: 24431.00000000, Mean: -0.01157895\n",
      "\n",
      "ğŸ” [Compare] Loading with torchaudio.load()\n",
      "âœ… [torchaudio] shape        : torch.Size([1, 38000]), sample_rate: 16000\n",
      "ğŸ“Š [torchaudio] Min: -0.7002, Max: 0.7456, Mean: -0.0000\n",
      "âœ… [normalize] Normalized waveform shape: torch.Size([1, 38000])\n",
      "ğŸ“Š [normalize] Normalized Min: -22944.00000000, Max: 24431.00000000, Mean: -0.01157895\n",
      "ğŸ“£ AED RAW   : â–ná»­aâ–vÃ²ngâ–trÃ¡iâ–Ä‘áº¥tâ–hÆ¡nâ–báº£yâ–nÄƒm<sos/eos>\n",
      "ğŸ“£ AED CLEAN : ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm<sos/eos>\n",
      "ğŸ†š So sÃ¡nh:\n",
      "- CTC:  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "- AED: ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm<sos/eos>\n",
      "ğŸŸ¢ Prediction     :  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "ğŸ”µ Ground Truth   : ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\n",
      "âŒ WER            : 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchaudio.compliance.kaldi import fbank\n",
    "from torch.utils.benchmark import Timer\n",
    "from jiwer import wer\n",
    "import os\n",
    "from chunkformer_vpb.model_utils import prepare_input_file, decode_long_form, get_default_args\n",
    "from chunkformer_vpb.model_utils import init, dump_module_structure, decode_aed_long_form\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = \"../../../chunkformer-large-vie\"  # adjust if needed\n",
    "output_dir = \"model_architect\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Load model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, char_dict = init(model_checkpoint, device)\n",
    "model.eval()\n",
    "\n",
    "args = get_default_args()\n",
    "args.audio_path = \"../../../debug_wavs/sample_00.wav\"\n",
    "args.label_text = \"ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\"\n",
    "feats = prepare_input_file(args.audio_path, device)\n",
    "decoded = decode_long_form(feats, model, char_dict, args, device)\n",
    "\n",
    "ctc_text = decoded\n",
    "aed_text_raw, aed_text_clean = decode_aed_long_form(feats, model, char_dict, args, device)\n",
    "\n",
    "print(f\"ğŸ†š So sÃ¡nh:\\n- CTC: {ctc_text}\\n- AED: {aed_text_clean}\")\n",
    "\n",
    "\n",
    "print(\"ğŸŸ¢ Prediction     :\", decoded)\n",
    "if args.label_text:\n",
    "    print(\"ğŸ”µ Ground Truth   :\", args.label_text)\n",
    "    print(\"âŒ WER            :\", wer(args.label_text.lower(), decoded.lower()))\n",
    "\n",
    "\n",
    "\n",
    "#   --model_checkpoint ../chunkformer-large-vie \\\n",
    "#   --audio_path ../debug_wavs/sample_00.wav \\\n",
    "#   --label_text \"ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf038ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kylh/.local/share/mamba/envs/stt310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¾ Loaded checkpoint from: ../../../chunkformer-large-vie/pytorch_model.bin\n",
      "ğŸ“¦ Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)\n",
      "ğŸ” AED decoder head included in checkpoint? âœ… YES\n",
      "ğŸ“Š Model total params: 113,852,240, trainable: 113,852,240\n",
      "!!!Vocab size: 6992\n",
      "\n",
      "ğŸ“¥ Loading file: ../../../debug_wavs/sample_00.wav\n",
      "ğŸ” [pydub] Raw frame_rate   : 16000\n",
      "ğŸ” [pydub] Sample width     : 4 bytes (32 bits)\n",
      "ğŸ” [pydub] Channels         : 1\n",
      "ğŸ” [pydub] Duration (ms)    : 2375 ms\n",
      "ğŸ§ª [pydub] Type of array     : <class 'array.array'>, dtype: int16\n",
      "ğŸ§ª [pydub] First 10 samples  : array('h', [0, 0, -1, 0, 0, 0, 1, 3, 9, 11])\n",
      "âœ… [pydub] Waveform shape    : torch.Size([1, 38000])\n",
      "ğŸ“Š [pydub] Min: -22944.00000000, Max: 24431.00000000, Mean: -0.01157895\n",
      "\n",
      "ğŸ” [Compare] Loading with torchaudio.load()\n",
      "âœ… [torchaudio] shape        : torch.Size([1, 38000]), sample_rate: 16000\n",
      "ğŸ“Š [torchaudio] Min: -0.7002, Max: 0.7456, Mean: -0.0000\n",
      "âœ… [normalize] Normalized waveform shape: torch.Size([1, 38000])\n",
      "ğŸ“Š [normalize] Normalized Min: -22944.00000000, Max: 24431.00000000, Mean: -0.01157895\n",
      ">>>>>>>>>>>>>>>>>>>  ys_out shape: torch.Size([1, 8]), logp shape: torch.Size([1, 8, 6992])\n",
      "Loss: 0.17942816019058228\n",
      "CTC Loss: 0.00239975075237453\n",
      "AED Loss: 0.2552974820137024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from chunkformer_vpb.training.finetune_utils import (\n",
    "    get_default_args,\n",
    "    prepare_input_file,\n",
    "    load_model_only,\n",
    "    GreedyTokenizer,\n",
    "    compute_chunkformer_loss,\n",
    ")\n",
    "\n",
    "def main():\n",
    "    # 1. Chuáº©n bá»‹ args vÃ  device\n",
    "    args = get_default_args()\n",
    "    args.model_checkpoint = \"../../../chunkformer-large-vie\"     # Ä‘Æ°á»ng dáº«n Ä‘áº¿n folder checkpoint\n",
    "    args.audio_path       = \"../../../debug_wavs/sample_00.wav\"  # file audio máº«u\n",
    "    args.label_text       = \"ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm\"  # label ground-truth\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 2. Load model + tokenizer\n",
    "    model, _ = load_model_only(args.model_checkpoint, device)\n",
    "    model.ctc_weight = 0.3\n",
    "    # model.reverse_weight = 0.3 -> can not work due to there is no right_decoder \n",
    "    tokenizer = GreedyTokenizer(vocab_path=f\"{args.model_checkpoint}/vocab.txt\")\n",
    "\n",
    "    # 3. Prepare input features\n",
    "    xs = prepare_input_file(args.audio_path, device)  # [1, T_raw, 80]\n",
    "\n",
    "    # 4. Compute loss\n",
    "    loss_dict = compute_chunkformer_loss(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        xs=xs,\n",
    "        args=args,\n",
    "        label_text=args.label_text,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # 5. In káº¿t quáº£\n",
    "    print(f\"Loss: {loss_dict['loss'].item()}\")\n",
    "    print(f\"CTC Loss: {loss_dict['loss_ctc'].item()}\")\n",
    "    print(f\"AED Loss: {loss_dict['loss_att'].item()}\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55473ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b88f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stt310)",
   "language": "python",
   "name": "stt310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
