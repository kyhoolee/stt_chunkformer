D∆∞·ªõi ƒë√¢y l√† m·ªôt v√≠ d·ª• v·ªÅ c√°ch b·∫°n c√≥ th·ªÉ ‚Äúl·∫Øp‚Äù v√†o `main_infer.py` ph·∫ßn t√≠nh CTC-loss v√† AED-loss ngay sau khi ƒë√£ extract ƒë∆∞·ª£c `encoder_outs_full` v√† `encoder_mask`, r·ªìi in ra ƒë·ªÉ debug. M√¨nh gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ s·∫µn:

* `args.label_text` l√† chu·ªói ground-truth
* h√†m `text2token` (d∆∞·ªõi) ƒë·ªÉ convert text‚Üítoken IDs theo `char_dict`
* `model.ctc` v√† `model._calc_att_loss` nh∆∞ trong WeNet

---

### 1. Th√™m helper: text ‚Üí token IDs

```python
def text2token(text: str, char_dict: Dict[int,str]):
    # ƒê·∫£o ng∆∞·ª£c char_dict
    token2char = {v:k for k,v in char_dict.items()}
    # Gi·∫£ s·ª≠ m·ªói k√Ω t·ª± ti·∫øng Vi·ªát ƒë√£ ƒë∆∞·ª£c split ƒë√∫ng subword
    ids = []
    for ch in text:
        if ch in token2char:
            ids.append(token2char[ch])
        # b·ªè qua k√Ω t·ª± kh√¥ng nh·∫≠n di·ªán
    return torch.tensor(ids, dtype=torch.long)
```

N·∫øu b·∫°n d√πng BPE subword, thay h√†m tr√™n b·∫±ng tokenizer c·ªßa WeNet ƒë·ªÉ align v·ªõi training.

---

### 2. T√≠nh Loss ngay trong inference

Ch√®n ƒëo·∫°n n√†y ngay sau khi b·∫°n c√≥

```python
# ƒë√£ c√≥:
#   encoder_outs_full: [1, T_out, D]
#   encoder_mask:      [1, 1, T_out]
#   args.label_text:   ground truth string
```

Th√™m:

```python
# --- prepare ground-truth ids + lens ---
if args.label_text:
    # 1) tokenize
    gt_ids = text2token(args.label_text, char_dict).to(device)  # [L]
    # 2) add sos/eos if c·∫ßn
    gt_ids = torch.cat([
        torch.tensor([model.sos], device=device), 
        gt_ids, 
        torch.tensor([model.eos], device=device)
    ])
    ys_pad = gt_ids.unsqueeze(0)                                  # [1, L]
    ys_lens = torch.tensor([ys_pad.size(1)], device=device)       # [1]

    # --- 1) CTC loss ---
    with torch.no_grad():
        loss_ctc, _ = model.ctc(
            encoder_outs_full,
            encoder_mask.squeeze(1).sum(1),
            ys_pad,
            ys_lens
        )
    loss_ctc = loss_ctc.item() if loss_ctc.numel()==1 else loss_ctc.sum().item()

    # --- 2) AED loss ---
    with torch.no_grad():
        loss_att, _ = model._calc_att_loss(
            encoder_outs_full,
            encoder_mask,
            ys_pad,
            ys_lens
        )
    loss_att = loss_att.item() if loss_att.numel()==1 else loss_att.sum().item()

    # --- hybrid ---
    ctc_weight = model.ctc_weight  # v√≠ d·ª• 0.3
    loss_hybrid = ctc_weight * loss_ctc + (1-ctc_weight) * loss_att

    print(f"üîç [DEBUG LOSS] CTC: {loss_ctc:.4f}, AED: {loss_att:.4f}, Hybrid: {loss_hybrid:.4f}")
```

---

### 3. V·ªã tr√≠ ch√®n

Trong h√†m `decode_long_form`, ngay sau:

```python
    encoder_outs_full = torch.cat(encoder_outs_chunks, dim=1)
    encoder_mask = torch.ones(1,1, encoder_outs_full.size(1), device=device)
```

b·∫°n th√™m block t√≠nh loss tr√™n, r·ªìi m·ªõi g·ªçi `model.decode_aed`.

---

### 4. K·∫øt qu·∫£ mong ƒë·ª£i

Khi ch·∫°y:

```bash
python main_infer.py --model_checkpoint ... --audio_path ... --label_text "n·ª≠a v√≤ng tr√°i ƒë·∫•t h∆°n b·∫£y nƒÉm"
```

B·∫°n s·∫Ω th·∫•y th√™m:

```
üîç [DEBUG LOSS] CTC: 1.2345, AED: 2.3456, Hybrid: 1.8765
```

‚Äî cho b·∫°n bi·∫øt ch√≠nh x√°c c√°c th√†nh ph·∫ßn loss c·ªßa model v·ªõi ƒëo·∫°n audio v√† ground-truth b·∫°n ƒë∆∞a v√†o, ƒë·ªÉ debug v√† hi·ªÉu s√¢u.
