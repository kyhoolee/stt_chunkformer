import torch
from torchaudio.compliance.kaldi import fbank
from torch.utils.benchmark import Timer
from jiwer import wer
import os
from chunkformer_vpb.model_utils import prepare_input_file, decode_long_form, get_default_args
from chunkformer_vpb.model_utils import init, dump_module_structure, decode_aed_long_form



model_checkpoint = "../../chunkformer-large-vie"  # adjust if needed
output_dir = "model_architect"
os.makedirs(output_dir, exist_ok=True)

# === Load model ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model, char_dict = init(model_checkpoint, device)
model.eval()

args = get_default_args()
args.audio_path = "../../debug_wavs/sample_00.wav"
args.label_text = "ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm"
feats = prepare_input_file(args.audio_path, device)
decoded = decode_long_form(feats, model, char_dict, args, device)

ctc_text = decoded
aed_text_raw, aed_text_clean = decode_aed_long_form(feats, model, char_dict, args, device)

print(f"ğŸ†š So sÃ¡nh:\n- CTC: {ctc_text}\n- AED: {aed_text_clean}")


print("ğŸŸ¢ Prediction     :", decoded)
if args.label_text:
    print("ğŸ”µ Ground Truth   :", args.label_text)
    print("âŒ WER            :", wer(args.label_text.lower(), decoded.lower()))



#   --model_checkpoint ../chunkformer-large-vie \
#   --audio_path ../debug_wavs/sample_00.wav \
#   --label_text "ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm"

-------------------

================= ğŸ§© [Encoder.forward_parallel_chunk] START =================
ğŸ“¥ Input shape: torch.Size([1, 236, 80]), xs_origin_lens: [236]
âš™ï¸ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200
ğŸ“ Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7
ğŸ”¹ Sample 0: original_len=236, padded_len=519, pad_frames=283, n_chunks=1, offset=0

ğŸ§± Total chunked xs shape: torch.Size([1, 519, 80])
ğŸ“ xs_lens (post chunk): torch.Size([1]), total_chunks: 1
âœ… Applied Global CMVN
ğŸ›ï¸ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])
ğŸ§® att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])
ğŸ“ Applied LayerNorm after encoder
ğŸ“¤ Final offset: [28]

âœ… [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [28], n_chunks: [1]
====================================================================

ğŸ“£ AED RAW   : â–ná»­aâ–vÃ²ngâ–trÃ¡iâ–Ä‘áº¥tâ–hÆ¡nâ–báº£yâ–nÄƒm<sos/eos>
ğŸ“£ AED CLEAN : ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm<sos/eos>
ğŸ†š So sÃ¡nh:
- CTC:  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm
- AED: ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm<sos/eos>
ğŸŸ¢ Prediction     :  ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm
ğŸ”µ Ground Truth   : ná»­a vÃ²ng trÃ¡i Ä‘áº¥t hÆ¡n báº£y nÄƒm
âŒ WER            : 0.0

-----------------------------


@torch.no_grad()
def decode_aed_long_form(xs, model, char_dict, args, device):
    chunk_size = args.chunk_size
    left_context_size = args.left_context_size
    right_context_size = args.right_context_size
    subsampling_factor = model.encoder.embed.subsampling_factor
    conv_kernel_size = model.encoder.cnn_module_kernel
    conv_lorder = conv_kernel_size // 2
    num_blocks = model.encoder.num_blocks
    hidden_dim = model.encoder._output_size
    attention_heads = model.encoder.attention_heads

    max_len = int(args.total_batch_duration // 0.01) // 2
    multiply_n = max_len // chunk_size // subsampling_factor
    truncated_context_size = chunk_size * multiply_n
    rel_right_context_size = (
        right_context_size + max(chunk_size, right_context_size) * (num_blocks - 1)
    ) * subsampling_factor

    # Init cache
    offset = torch.zeros(1, dtype=torch.int, device=device)
    att_cache = torch.zeros((num_blocks, left_context_size, attention_heads, hidden_dim * 2 // attention_heads), device=device)
    cnn_cache = torch.zeros((num_blocks, hidden_dim, conv_lorder), device=device)

    encoder_outs_chunks = []
    encoder_lens_total = 0

    for idx in range(0, xs.shape[1], truncated_context_size * subsampling_factor):
        start = truncated_context_size * subsampling_factor * idx
        end = min(start + truncated_context_size * subsampling_factor + 7, xs.shape[1])

        x = xs[:, start:end + rel_right_context_size]
        x_len = torch.tensor([x[0].shape[0]], dtype=torch.int, device=device)

        encoder_outs, encoder_lens, _, att_cache, cnn_cache, offset = model.encoder.forward_parallel_chunk(
            xs=x, 
            xs_origin_lens=x_len,
            chunk_size=chunk_size,
            left_context_size=left_context_size,
            right_context_size=right_context_size,
            att_cache=att_cache,
            cnn_cache=cnn_cache,
            truncated_context_size=truncated_context_size,
            offset=offset
        )

        encoder_outs = encoder_outs[:, :encoder_lens]
        encoder_outs_chunks.append(encoder_outs)
        encoder_lens_total += encoder_outs.shape[1]

        if start + rel_right_context_size >= xs.shape[1]:
            break

    # Cat all chunk outputs
    encoder_outs_full = torch.cat(encoder_outs_chunks, dim=1)
    encoder_mask = torch.ones(1, 1, encoder_outs_full.shape[1], dtype=torch.bool, device=device)

    # Decode AED
    pred_token_ids, _ = model.decode_aed(encoder_outs_full, encoder_mask, maxlen=100)
    pred_ids = pred_token_ids[0].tolist()  # Äá»•i tÃªn cho ngáº¯n gá»n

    # Raw vs Clean
    raw = transcript_raw(pred_ids, char_dict)
    clean = transcript_clean(pred_ids, char_dict)

    print(f"ğŸ“£ AED RAW   : {raw}")
    print(f"ğŸ“£ AED CLEAN : {clean}")

    return raw, clean

-------------------------
from wenet.transformer.decoder import BiTransformerDecoder

@torch.no_grad()
def init(model_checkpoint, device):
    config_path = os.path.join(model_checkpoint, "config.yaml")
    checkpoint_path = os.path.join(model_checkpoint, "pytorch_model.bin")
    symbol_table_path = os.path.join(model_checkpoint, "vocab.txt")

    # 1. Load config
    with open(config_path, 'r') as fin:
        config = yaml.load(fin, Loader=yaml.FullLoader)

    # 2. Init model (encoder + ctc)
    model = init_model(config, config_path)
    model.eval()

    # 3. Load full checkpoint
    ckpt = torch.load(checkpoint_path, map_location="cpu")
    load_checkpoint(model, checkpoint_path)

    # 4. Add decoder (AED) if exists in checkpoint
    if any(k.startswith("decoder.left_decoder") for k in ckpt):
        vocab_size = ckpt["decoder.left_decoder.embed.0.weight"].size(0)
        d_model = ckpt["decoder.left_decoder.after_norm.weight"].size(0)
        block_ids = {int(k.split('.')[3]) for k in ckpt if k.startswith("decoder.left_decoder.decoders.")}
        num_blocks = max(block_ids) + 1

        decoder = BiTransformerDecoder(
            vocab_size=vocab_size,
            encoder_output_size=d_model,
            attention_heads=8,
            linear_units=2048,
            num_blocks=num_blocks,
            r_num_blocks=0,
            dropout_rate=0.1,
            positional_dropout_rate=0.1,
            self_attention_dropout_rate=0.0,
            src_attention_dropout_rate=0.0,
            input_layer="embed",
            use_output_layer=True,
            normalize_before=True,
            src_attention=True,
            query_bias=True,
            key_bias=True,
            value_bias=True,
            activation_type="relu",
            gradient_checkpointing=False,
            tie_word_embedding=False,
            use_sdpa=False,
            layer_norm_type='layer_norm',
            norm_eps=1e-5,
            n_kv_head=None,
            head_dim=None,
            mlp_type='position_wise_feed_forward',
            mlp_bias=True,
            n_expert=8,
            n_expert_activated=2
        )

        # âœ… Load Ä‘Ãºng pháº§n left_decoder
        decoder.left_decoder.load_state_dict({
            k[len("decoder.left_decoder."):]: v
            for k, v in ckpt.items()
            if k.startswith("decoder.left_decoder.")
        }, strict=False)

        decoder = decoder.to(device)
        decoder.eval()

        model.decoder = decoder  # Attach to main model

    # 5. Move parts to device
    model.encoder = model.encoder.to(device)
    model.ctc = model.ctc.to(device)

    # 6. Build char dictionary
    symbol_table = read_symbol_table(symbol_table_path)
    char_dict = {v: k for k, v in symbol_table.items()}

    return model, char_dict



-------------------------
from collections import defaultdict
from typing import Dict, List, Optional, Tuple

import torch
import logging

from torch.nn.utils.rnn import pad_sequence
from .ctc import CTC
from .utils.common import (IGNORE_ID, add_sos_eos, log_add,
                                remove_duplicates_and_blank, th_accuracy,
                                reverse_pad_list)
# from .mwer import MWER
from .utils.mask import (make_pad_mask, mask_finished_preds,
                              mask_finished_scores, subsequent_mask)


class ASRModel(torch.nn.Module):
    """CTC-attention hybrid Encoder-Decoder model"""
    def __init__(
        self,
        vocab_size: int,
        encoder,
        ctc: CTC,
        ctc_weight: float = 0.5,
        ignore_id: int = IGNORE_ID,
        reverse_weight: float = 0.0,
        decoder=None
    ):
        assert 0.0 <= ctc_weight <= 1.0, ctc_weight

        super().__init__()
        # note that eos is the same as sos (equivalent ID)
        self.sos = vocab_size - 1
        self.eos = vocab_size - 1
        self.vocab_size = vocab_size
        self.ignore_id = ignore_id
        self.ctc_weight = ctc_weight
        self.reverse_weight = reverse_weight

        self.encoder = encoder
        self.encoder.ctc = ctc
        self.ctc = ctc        

        self.decoder = decoder

    def decode_aed(self, encoder_out: torch.Tensor, encoder_mask: torch.Tensor, maxlen: int = 100):
        """
        Greedy AED decoding:  
          - encoder_out: [B, T_in, D]  
          - encoder_mask: [B, 1, T_in]  
        Tráº£ vá»:  
          - token IDs [B, T_out], score sequence [B, T_out, V]
        """
        batch_size = encoder_out.size(0)
        # Khá»Ÿi táº¡o ys vá»›i sos
        ys = encoder_out.new_full((batch_size, 1), fill_value=self.sos, dtype=torch.long)
        scores = []

        for _ in range(maxlen):
            # Táº¡o tensor Ä‘á»™ dÃ i cá»§a ys
            ys_lens = torch.tensor([ys.size(1)] * batch_size, dtype=torch.long, device=ys.device)
            # Gá»i decoder vá»›i Ä‘Ãºng thá»© tá»± args
            l_x, r_x, olens = self.decoder(
                encoder_out,    # memory
                encoder_mask,   # memory_mask
                ys,             # ys_in_pad
                ys_lens,        # ys_in_lens
                None,           # r_ys_in_pad (khÃ´ng dÃ¹ng reverse here)
                0.0,            # reverse_weight
            )
            # l_x: [B, T_cur, V]
            # Láº¥y logits cá»§a token cuá»‘i cÃ¹ng
            prob = torch.nn.functional.log_softmax(l_x[:, -1], dim=-1)
            next_token = prob.argmax(dim=-1, keepdim=True)  # [B,1]
            ys = torch.cat([ys, next_token], dim=1)         # [B, T_cur+1]
            scores.append(prob)

            # Náº¿u táº¥t cáº£ batch Ä‘á»u sinh eos thÃ¬ dá»«ng sá»›m
            if (next_token == self.eos).all():
                break

        # Tráº£ vá» sequence bá» sos + scores
        return ys[:, 1:], torch.stack(scores, dim=1)


---------------------------

Báº¡n Ä‘Ã£ support mÃ¬nh bá»• sung pháº§n AED 
- Sá»­ dá»¥ng wenet Ä‘á»ƒ dÃ¹ng Ä‘Ãºng BiTransformerDecoder
- Bá»• sung decode_aed vÃ o ASR model 
- DÃ¹ng decode_aed vÃ  viáº¿t full method Ä‘á»ƒ transcript cho 1 audio tá»« output cá»§a AED - káº¿t quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i CTC 

-> Giá» báº¡n support mÃ¬nh viáº¿t pháº§n TÃ­nh Loss Ä‘Ãºng theo Ä‘á»‹nh nghÄ©a cá»§a chunkformer 
- Ä‘Ãºng cÃ¡ch tÃ­nh 
- import logic tá»« wenet -> háº¡n cháº¿ viáº¿t láº¡i Ä‘á»ƒ Ä‘áº£m báº£o tÆ°Æ¡ng Ä‘Æ°Æ¡ng 
- dÃ¹ng Ä‘Ãºng config 