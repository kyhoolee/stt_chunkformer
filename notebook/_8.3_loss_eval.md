Hai con số này nằm trong vùng “hợp lý” với cách bạn đang tính:

* **CTC Loss ≈ 6.56**

  * Vì bạn đang dùng `reduction="mean"` trên toàn bộ frame, tức loss = $-\frac1N\sum_{t=1}^N\log p_{t}(y_t)$. Với khoảng 60 frame (đầu ra) và một audio ngắn, giá trị \~6 là bình thường — tương đương xấp xỉ 6 nats (hoặc ≈8.7 bits) “bất ngờ” trung bình mỗi frame.
  * Trong quá trình fine-tune, bạn sẽ thấy CTC loss giảm dần từ \~6 xuống tầm 1–2.

* **AED Loss ≈ 0.26**

  * Đây là giá trị KL-divergence trung bình sau label-smoothing (với $\varepsilon=0.1$) trên mỗi token. Một loss \~0.2–0.4 là phổ biến khi decoder đã khá ổn từ checkpoint gốc.
  * Khi train thêm, bạn sẽ thấy nó tiếp tục đi xuống tầm 0.05–0.1 khi model “quen” domain mới.

---

### Về tổng loss = 0.3·CTC + 0.7·AED ≈ 2.15

* Con số \~2 cũng nằm trong mức mong đợi cho một sequence đầu tiên từ checkpoint gốc.
* Mục tiêu fine-tune là **đẩy tổng loss giảm** theo thời gian (về <1.0–1.5) đồng thời theo dõi WER trên validation để dừng sớm.

---

**Kết luận**:

> ✔️ Các giá trị CTC \~(5–7) và AED \~(0.1–0.5) cho audio đầu tiên là “bình thường”.
> ✔️ Quan trọng nhất là chúng phải **giảm đều** qua mỗi epoch khi bạn fine-tune. Nếu CTC vẫn quanh 6 và AED ko xuống nữa, có thể cần giảm learning rate hoặc thêm data augmentation.
