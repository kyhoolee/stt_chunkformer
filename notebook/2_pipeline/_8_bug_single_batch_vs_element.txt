
# ==================== LOSS COMPUTE UTILS ====================

import os
import torch
import torch.nn.functional as F
from typing import Tuple

DEBUG_LOSS = bool(int(os.getenv("DEBUG_LOSS", "1")))   # 1 → in log

def encode_offline_batch_debug(
    feats: torch.Tensor,       # [B, T_raw, D]
    feat_lens: torch.Tensor,   # [B]
    model,
    chunk_cfg,
    device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Như encode_offline_batch nhưng chèn debug prints:
      - In shape đầu vào
      - In shape sau mỗi sample
      - In pad time và mask pad (chỉ khi B > 1)
      - In shape cuối của enc_outs và enc_masks
    """
    B, T_raw, D = feats.shape
    print(f"[ENC_OFFLINE] start: B={B}, T_raw_max={T_raw}, D={D}")

    outs, masks, lengths = [], [], []
    for i in range(B):
        x_i = feats[i : i+1].to(device)       # [1, T_raw, D]
        l_i = feat_lens[i].item()
        print(f"  [sample {i}] raw_feat.shape = {x_i.shape}, raw_len = {l_i}")

        out_i, m_i = _chunk_encoder_forward(x_i, model, chunk_cfg, device)
        # out_i: [1, T_i, D], m_i: [1,1,T_i]
        T_i = out_i.size(1)
        print(f"    → after chunk-fwd: out_i.shape = {out_i.shape}, mask_i.shape = {m_i.shape}")

        outs .append(out_i)
        masks.append(m_i)
        lengths.append(T_i)

    # Nếu chỉ có 1 sample, không pad thêm
    if B == 1:
        print("[ENC_OFFLINE] B=1, skip pad → return single sample as is")
        enc_outs = outs[0]       # [1, T_0, D]
        enc_masks= masks[0]      # [1, 1, T_0]
        print(f"[ENC_OFFLINE] final enc_outs.shape = {enc_outs.shape}, enc_masks.shape = {enc_masks.shape}")
        return enc_outs, enc_masks

    # Ngược lại, pad time dim để nối batch
    T_max = max(lengths)
    print(f"[ENC_OFFLINE] pad to T_max = {T_max}")

    padded_outs, padded_masks = [], []
    for i, (out_i, m_i, L) in enumerate(zip(outs, masks, lengths)):
        pad_t = T_max - L
        out_pad  = F.pad(out_i, (0, 0, 0, pad_t))       # pad time ở chiều thứ 1
        mask_pad = F.pad(m_i,  (0, pad_t), value=0)     # pad mask ở cuối time
        print(f"  [sample {i}] pad_t={pad_t}, out_pad.shape={out_pad.shape}, mask_pad.shape={mask_pad.shape}")
        padded_outs .append(out_pad)
        padded_masks.append(mask_pad)

    enc_outs  = torch.cat(padded_outs,  dim=0)         # [B, T_max, D]
    enc_masks = torch.cat(padded_masks, dim=0)         # [B, 1, T_max]
    print(f"[ENC_OFFLINE] final enc_outs.shape = {enc_outs.shape}, enc_masks.shape = {enc_masks.shape}")

    return enc_outs, enc_masks


def compute_loss_batch_v1(
    model: ASRModel,
    feats: torch.Tensor,      # [B, T_raw, D]
    feat_lens: torch.Tensor,  # [B]
    toks: torch.Tensor,       # [B, L_pad]   (no sos/eos)
    tok_lens: torch.Tensor,   # [B]
    cfg,
    device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:

    # 1) Offline‐encode cả batch
    enc_outs, enc_masks = encode_offline_batch_debug(feats, feat_lens, model, cfg.chunk, device)
    # enc_outs: [B, T_enc, D], enc_masks: [B,1,T_enc]
    enc_lens = enc_masks.squeeze(1).sum(1).to(torch.long)  # [B]

    if DEBUG_LOSS:
        print(f"[DBG] enc_outs {enc_outs.shape}, enc_lens {enc_lens.tolist()}")

    # 2) CTC loss (không sos/eos)
    #   model.ctc expects: (logp, input_lengths, targets, target_lengths)
    loss_ctc, _ = model.ctc(
        enc_outs, enc_lens,
        toks.to(device), tok_lens.to(device)
    )
    # sum across batch
    loss_ctc = loss_ctc.sum()

    # 3) AED loss (cần sos/eos)
    # build ys_pad, ys_lens cho batch
    sos_id, eos_id = model.sos, model.eos
    # create [B, L+2] and lengths [B]
    ys_pad = []
    ys_lens = []
    for i in range(feats.size(0)):
        raw = toks[i, : tok_lens[i]].tolist()
        seq = [sos_id] + raw + [eos_id]
        ys_pad.append(torch.tensor(seq, dtype=torch.long, device=device))
        ys_lens.append(len(seq))
    ys_pad = torch.nn.utils.rnn.pad_sequence(ys_pad, batch_first=True, padding_value=model.ignore_id)  # [B, L_max+2]
    ys_lens = torch.tensor(ys_lens, dtype=torch.long, device=device)                                 # [B]

    loss_att, _ = model._calc_att_loss(enc_outs, enc_masks, ys_pad, ys_lens)
    loss_att = loss_att.sum()

    # 4) Hybrid
    ctc_w = cfg.model.ctc_weight
    loss = ctc_w * (loss_ctc / feats.size(0)) + (1 - ctc_w) * (loss_att / feats.size(0))

    return loss, (loss_ctc / feats.size(0)), (loss_att / feats.size(0))


---------------------


def compute_chunkformer_loss(model: ASRModel,
                             tokenizer: GreedyTokenizer,
                             xs: torch.Tensor,
                             args,
                             label_text: str,
                             device) -> dict:
    """
    xs: [1, T_raw, 80] input features
    args: arg namespace with chunk_size, left_context_size, right_context_size, total_batch_duration
    label_text: ground-truth string
    Returns: dict {loss, loss_ctc, loss_att}
    """
    # 1) forward chunk-encoder
    encoder_outs_full, encoder_mask = _chunk_encoder_forward(xs, model, args, device)
    # encoder_lens = encoder_mask.squeeze(1).sum(2)  # [1]
    # encoder_mask: [1, 1, T] → squeeze → [1, T], sum over time dim → [1]
    encoder_lens = encoder_mask.squeeze(1).sum(1).to(torch.long)

    # 2) text -> labels
    ys_pad, ys_lens = tokenizer.text2labels(label_text)
    ys_pad = ys_pad.to(device)
    ys_lens = ys_lens.to(device).to(torch.long)
    # @NOTE: 
    # - Phần chèn sos và eos này chỉ dùng cho AED loss 
    # - Không dùng cho CTC loss -> add thêm vào tăng giá trị không cần thiết của CTC loss
    # - Dùng thì loss vẫn giảm dần -> nhưng không đúng với CTC loss

    # 3) CTC loss
    loss_ctc, _ = model.ctc(
        encoder_outs_full,
        encoder_lens,
        ys_pad,
        ys_lens
    )
    # 4) AED loss
    loss_att, _ = model._calc_att_loss(
        encoder_outs_full,
        encoder_mask,
        ys_pad,
        ys_lens
    )

    # sum if needed
    loss_ctc = loss_ctc.sum() if loss_ctc.dim()>0 else loss_ctc
    loss_att = loss_att.sum() if loss_att.dim()>0 else loss_att

    # 5) hybrid
    ctc_w = model.ctc_weight
    loss = ctc_w*loss_ctc + (1-ctc_w)*loss_att

    return {"loss": loss,
            "loss_ctc": loss_ctc,
            "loss_att": loss_att}


-------------------------



import torch
from chunkformer_vpb.training.finetune_utils import (
    get_default_args,
    prepare_input_file,
    load_model_only,
    GreedyTokenizer,
    compute_chunkformer_loss,
)
from chunkformer_vpb.training.data_loader import normalize_vi

import os, math, torch
from chunkformer_vpb.training.finetune_config import FinetuneConfig
from chunkformer_vpb.training.data_loader     import get_dataloaders
from chunkformer_vpb.training.optimizer       import build_model_and_optimizer
from chunkformer_vpb.training.finetune_utils  import compute_loss_batch_v1, compute_loss_batch_v2


CFG_PATH   = "../../config/finetune_config.yaml"

def debug_text_pipeline(label_text: str, tokenizer: GreedyTokenizer):
    print(">>> Text pipeline debug <<<")
    print("  Original text :", label_text)
    norm = normalize_vi(label_text)
    print("  Normalized   :", norm)
    ids = tokenizer.tokenize(norm)
    print("  Token IDs    :", ids)
    try:
        dec = tokenizer.decode_ids(ids)
    except AttributeError:
        # nếu chưa có decode_ids
        dec = "".join(tokenizer.vocab[id] for id in ids)
    print("  Decoded      :", dec)
    print()

def main():
    # 1) chuẩn bị args + device
    args = get_default_args()
    args.model_checkpoint = "../../../chunkformer-large-vie"
    args.audio_path       = "../../../debug_wavs/utt_000664.wav"
    args.label_text       = "một giọng nói du dương không thể lẫn với ai khác cất lên"

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 2) load model + tokenizer
    model, _ = load_model_only(args.model_checkpoint, device)
    model.ctc_weight = 0.3
    tokenizer = GreedyTokenizer(vocab_path=f"{args.model_checkpoint}/vocab.txt")

    # 3) debug text pipeline (single)
    debug_text_pipeline(args.label_text, tokenizer)

    # 4) prepare single‐sample features + loss
    xs = prepare_input_file(args.audio_path, device)  # [1, T_raw, 80]
    print(">>> Single‐sample loss <<<")
    loss_s = compute_chunkformer_loss(
        model=model,
        tokenizer=tokenizer,
        xs=xs,
        args=args,
        label_text=args.label_text,
        device=device
    )
    print(f"Single: loss={loss_s['loss']:.3f},  ctc={loss_s['loss_ctc']:.3f}, att={loss_s['loss_att']:.3f}\n")

    # 5) now wrap into a batch of size 1 and run batch version
    #    (giả sử bạn có hàm compute_loss_batch_v1 cho batch)
    from chunkformer_vpb.training.finetune_utils import compute_loss_batch_v1
    feats = xs
    feat_lens = torch.tensor([xs.shape[1]], device=device)
    # build toks/tok_lens exactly như in DataLoader
    norm = normalize_vi(args.label_text)
    ids = tokenizer.tokenize(norm)
    toks = torch.LongTensor([ids]).to(device)
    tok_lens = torch.tensor([len(ids)], device=device)


    cfg = FinetuneConfig.from_yaml(CFG_PATH)

    # để nhanh: batch nhỏ & shuffle false
    cfg.training.batch_size = 1
    cfg.training.shuffle    = False

    print(">>> Batch‐of‐1 loss <<<")
    loss_b, loss_b_ctc, loss_b_att = compute_loss_batch_v1(
        model, feats, feat_lens, toks, tok_lens, cfg, device
    )
    print(f"Batch1: loss={loss_b:.3f},  ctc={loss_b_ctc:.3f}, att={loss_b_att:.3f}")

if __name__ == "__main__":
    main()

-----------------------



/home/kylh/.local/share/mamba/envs/stt310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
[2025-07-16 15:45:42] INFO: Checkpoint: loading from checkpoint ../../../chunkformer-large-vie/pytorch_model.bin for GPU

🧾 Loaded checkpoint from: ../../../chunkformer-large-vie/pytorch_model.bin
📦 Checkpoint keys: ['encoder.global_cmvn.mean', 'encoder.global_cmvn.istd', 'encoder.embed.out.weight', 'encoder.embed.out.bias', 'encoder.embed.conv.0.weight'] ... (total 813)
🔍 AED decoder head included in checkpoint? ✅ YES
📊 Model total params: 113,852,240, trainable: 113,852,240
>>> Text pipeline debug <<<
  Original text : một giọng nói du dương không thể lẫn với ai khác cất lên
  Normalized   : một giọng nói du dương không thể lẫn với ai khác cất lên
  Token IDs    : [4104, 2648, 4564, 2300, 2356, 3305, 5697, 3688, 6411, 1333, 3277, 2146, 3597]
  Decoded      : một giọng nói du dương không thể lẫn với ai khác cất lên

>>> Single‐sample loss <<<

================= 🧩 [Encoder.forward_parallel_chunk] START =================
📥 Input shape: torch.Size([1, 423, 80]), xs_origin_lens: [423]
⚙️ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200
📏 Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7

🧱 Total chunked xs shape: torch.Size([1, 519, 80])
📐 xs_lens (post chunk): torch.Size([1]), total_chunks: 1
🎛️ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])
🧮 att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])
🧩 Layer 0: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 1: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 2: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 3: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 4: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 5: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 6: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 7: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 8: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 9: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 10: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 11: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 12: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 13: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 14: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 15: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 16: xs shape after layer = torch.Size([1, 64, 512])
📏 Applied LayerNorm after encoder
📤 Final offset: [52]

✅ [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [52], n_chunks: [1]
====================================================================

>>>>>>>>>>>>>>>>>>>  ys_out shape: torch.Size([1, 14]), logp shape: torch.Size([1, 14, 6992])
Single: loss=1.597,  ctc=3.890, att=0.614

>>> Batch‐of‐1 loss <<<
[ENC_OFFLINE] start: B=1, T_raw_max=423, D=80
  [sample 0] raw_feat.shape = torch.Size([1, 423, 80]), raw_len = 423

================= 🧩 [Encoder.forward_parallel_chunk] START =================
📥 Input shape: torch.Size([1, 423, 80]), xs_origin_lens: [423]
⚙️ chunk_size=64, left_context=128, right_context=128, truncated_context_size=11200
📏 Subsampling: 8, Chunk frame size: 519, Step: 512, Conv lorder: 7

🧱 Total chunked xs shape: torch.Size([1, 519, 80])
📐 xs_lens (post chunk): torch.Size([1]), total_chunks: 1
🎛️ Embedded xs shape: torch.Size([1, 64, 512]), PosEmb shape: torch.Size([1, 383, 512])
🧮 att_mask shape: torch.Size([1, 1, 320]), mask_pad shape: torch.Size([1, 1, 78])
🧩 Layer 0: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 1: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 2: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 3: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 4: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 5: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 6: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 7: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 8: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 9: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 10: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 11: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 12: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 13: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 14: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 15: xs shape after layer = torch.Size([1, 64, 512])
🧩 Layer 16: xs shape after layer = torch.Size([1, 64, 512])
📏 Applied LayerNorm after encoder
📤 Final offset: [52]

✅ [Encoder Output] xs: torch.Size([1, 64, 512]), xs_lens: [52], n_chunks: [1]
====================================================================

    → after chunk-fwd: out_i.shape = torch.Size([1, 52, 512]), mask_i.shape = torch.Size([1, 1, 52])
[ENC_OFFLINE] B=1, skip pad → return single sample as is
[ENC_OFFLINE] final enc_outs.shape = torch.Size([1, 52, 512]), enc_masks.shape = torch.Size([1, 1, 52])
[DBG] enc_outs torch.Size([1, 52, 512]), enc_lens [52]
>>>>>>>>>>>>>>>>>>>  ys_out shape: torch.Size([1, 14]), logp shape: torch.Size([1, 14, 6992])
Batch1: loss=0.430,  ctc=0.001, att=0.614