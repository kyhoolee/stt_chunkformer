#!/usr/bin/env python3
"""
fine_tune_main.py â€“ Pipeline huáº¥n luyá»‡n Ä‘áº§y Ä‘á»§:
â€¢ Load config vÃ  mÃ´ hÃ¬nh
â€¢ Cháº¡y training theo sá»‘ epoch
â€¢ LÆ°u checkpoint má»—i epoch
â€¢ TÃ­nh WER trÃªn dev set sau má»—i epoch
"""

import os, argparse, torch, yaml
import time
from jiwer import wer

from .finetune_config import FinetuneConfig
from .data_loader     import get_dataloaders, get_dataloaders_smoke
from .optimizer       import build_model_and_optimizer
from .finetune_utils  import compute_loss_batch_v1, _chunk_encoder_forward

torch.autograd.set_detect_anomaly(True)   # phÃ¡t hiá»‡n NaN náº¿u cÃ³

# ======== ARGPARSE ========
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", required=True, help="Path to finetune_config.yaml")
    parser.add_argument("--smoke", action="store_true", help="DÃ¹ng subset nhá» Ä‘á»ƒ debug nhanh")
    parser.add_argument("--smoke-ratio", type=float, default=0.01, help="Tá»· lá»‡ data dÃ¹ng cho smoke test (máº·c Ä‘á»‹nh 0.01)")
    return parser.parse_args()


# ======== TRAIN LOOP ========

def train():
    args = parse_args()
    cfg_path = args.config
    smoke = args.smoke
    smoke_ratio = args.smoke_ratio
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    run_train(cfg_path, smoke=smoke, smoke_ratio=smoke_ratio, device=device)

def freeze_encoder_groups(model, config):
    """
    config: FreezeConfigFT dataclass
    """
    if config.cmvn:
        for param in model.encoder.global_cmvn.parameters():
            param.requires_grad = False

    if config.subsampling:
        for param in model.encoder.embed.parameters():
            param.requires_grad = False

    if config.post_embed_norm:
        for param in model.encoder.after_norm.parameters():
            param.requires_grad = False

    if config.encoder_layers > 0:
        freeze_n = config.encoder_layers
        for i, layer in enumerate(model.encoder.encoders):
            if i < freeze_n:
                for param in layer.parameters():
                    param.requires_grad = False

    if config.ctc:
        for param in model.encoder.ctc.parameters():
            param.requires_grad = False


# ======== RUN TRAIN ========

def run_train(cfg_path, smoke=False, smoke_ratio=0.01,
              eval_train=False, eval_ratio=0.1,
              device="cpu", resume_ckpt_path=None):

    cfg = FinetuneConfig.from_yaml(cfg_path)

    # === Load dataloader ===
    if smoke:
        print(f"âš™ï¸  Running in smoke mode with ratio={smoke_ratio}")
        train_loader, dev_loader = get_dataloaders_smoke(cfg, ratio=smoke_ratio)
    else:
        train_loader, dev_loader = get_dataloaders(cfg)

    total_steps = len(train_loader) * cfg.training.epochs
    model, tokenizer, optimizer, scheduler = build_model_and_optimizer(cfg, device, total_steps)

    # === Resume checkpoint náº¿u cÃ³ ===
    if resume_ckpt_path and os.path.exists(resume_ckpt_path):
        print(f"â™»ï¸ Resume training tá»« checkpoint: {resume_ckpt_path}")
        ckpt = torch.load(resume_ckpt_path, map_location=device)
        model.load_state_dict(ckpt["model_state_dict"])
        optimizer.load_state_dict(ckpt["optimizer_state_dict"])
        scheduler.load_state_dict(ckpt["scheduler_state_dict"])
        start_epoch = ckpt.get("epoch", 0) + 1
        global_step = ckpt.get("global_step", 0)
    else:
        print(f"ğŸ”° Khá»Ÿi táº¡o tá»« Ä‘áº§u (pretrained model)")
        start_epoch = 1
        global_step = 0

    model.to(device)

    # === Freeze encoder náº¿u cáº§n ===
    if hasattr(cfg, "freeze") and cfg.freeze is not None:
        freeze_encoder_groups(model, cfg.freeze)

    # === ThÃ´ng tin thiáº¿t bá»‹ ===
    print(f"ğŸ’» Sá»­ dá»¥ng thiáº¿t bá»‹: {device}")
    model_device = next(model.parameters()).device
    if model_device != device:
        print(f"âš ï¸ MÃ´ hÃ¬nh khÃ´ng á»Ÿ thiáº¿t bá»‹ Ä‘Ãºng! ({model_device})")
    else:
        print(f"âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn sang Ä‘Ãºng thiáº¿t bá»‹: {device}")

    # === Evaluate trÆ°á»›c training ===
    print("\nğŸ§ª ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÆ°á»›c khi fine-tune:")
    if dev_loader is not None:
        evaluate(model, tokenizer, dev_loader, cfg, device)
    else:
        print("âš ï¸ KhÃ´ng cÃ³ dá»¯ liá»‡u dev Ä‘á»ƒ Ä‘Ã¡nh giÃ¡!")

    if eval_train:
        print("\nğŸ§ª ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p train:")
        evaluate(model, tokenizer, train_loader, cfg, device, eval_ratio=eval_ratio)

    # === Train loop ===
    num_steps_per_epoch = len(train_loader)

    for epoch in range(start_epoch, cfg.training.epochs + 1):
        model.train()
        print(f"\nğŸŒ€ Epoch {epoch} báº¯t Ä‘áº§u...")
        epoch_start_time = time.time()

        for step, (feats, feat_lens, toks, tok_lens) in enumerate(train_loader, 1):
            step_start_time = time.time()

            feats, feat_lens = feats.to(device), feat_lens.to(device)
            toks,  tok_lens  = toks.to(device),  tok_lens.to(device)

            loss, loss_ctc, loss_att = compute_loss_batch_v1(
                model, feats, feat_lens, toks, tok_lens, cfg, device
            )

            optimizer.zero_grad()
            loss.backward()
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.max_grad_norm)
            optimizer.step()
            scheduler.step()
            global_step += 1

            step_time = time.time() - step_start_time
            remaining_steps = num_steps_per_epoch - step
            eta_epoch = remaining_steps * step_time
            eta_min, eta_sec = divmod(int(eta_epoch), 60)

            lr_now = scheduler.get_last_lr()[0]
            print(f"[Epoch {epoch} Step {step}/{num_steps_per_epoch} | "
                  f"Global-Step {global_step}] "
                  f"loss={loss.item():.4f} (ctc={loss_ctc.item():.4f}, att={loss_att.item():.4f}) "
                  f"grad={grad_norm:.2f}  lr={lr_now:.2e} "
                  f"| â±ï¸ {step_time:.2f}s/step - ETA: {eta_min}m{eta_sec}s")

        epoch_duration = time.time() - epoch_start_time
        ep_m, ep_s = divmod(int(epoch_duration), 60)
        print(f"âœ… Epoch {epoch} hoÃ n táº¥t trong {ep_m}m{ep_s}s")

        # === Save checkpoint (Ä‘áº§y Ä‘á»§ state) ===
        ckpt_dir = cfg.training.checkpoint_dir
        os.makedirs(ckpt_dir, exist_ok=True)
        ckpt_path = os.path.join(ckpt_dir, f"epoch{epoch}.pt")
        torch.save({
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "scheduler_state_dict": scheduler.state_dict(),
            "epoch": epoch,
            "global_step": global_step,
        }, ckpt_path)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u checkpoint: {ckpt_path}")

        # === Evaluate dev ===
        evaluate(model, tokenizer, dev_loader, cfg, device)


# ======== EVALUATE BUCKETS ========
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Eval WER + CER + Char Confusions theo SNR bucket cho manifest JSON array.

Usage:
  python eval_report.py \
    --meta manifest_user_only.with_snr.json \
    --bucket-field snr_bucket \
    --ref-field text \
    --hyp-field base_text \
    --exclude-buckets noisy \
    --conf-topn 20 \
    --cer-strip-spaces

Requires:
  pip install jiwer
"""
import json
import math
import re
import argparse
from collections import defaultdict, Counter
from typing import List, Tuple, Dict, Any, Optional
from jiwer import wer as jiwer_wer

# =============== Text normalization (nháº¹) ===============
def _norm(s: str) -> str:
    s = (s or "").lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s

# =============== CER & Alignment utils =================
def _cer_value(ref: str, hyp: str, strip_spaces: bool = True) -> float:
    """
    CER = edit_distance(chars) / len(ref_chars). Khoáº£ng tráº¯ng máº·c Ä‘á»‹nh bá».
    """
    r = ref.replace(" ", "") if strip_spaces else ref
    h = hyp.replace(" ", "") if strip_spaces else hyp
    if len(r) == 0:
        return 1.0 if len(h) > 0 else 0.0
    _, dist = _align_chars(r, h)  # dist = edit distance
    return dist / len(r)

def _align_chars(ref: str, hyp: str) -> Tuple[List[Tuple[Optional[str], Optional[str]]], int]:
    """
    Tráº£ vá»:
      - path: danh sÃ¡ch cáº·p (r_char | None, h_char | None)
              None á»Ÿ 1 phÃ­a thá»ƒ hiá»‡n delete/insert
      - dist: Levenshtein distance (S + D + I)
    Thuáº­t toÃ¡n: DP chuáº©n (O(len(ref)*len(hyp))) vá»›i truy váº¿t.
    """
    R, H = list(ref), list(hyp)
    n, m = len(R), len(H)
    # dp[i][j] = distance for R[:i], H[:j]
    dp = [[0]*(m+1) for _ in range(n+1)]
    bt = [[(0,0)]*(m+1) for _ in range(n+1)]  # backtrace: from where

    for i in range(1, n+1):
        dp[i][0] = i
        bt[i][0] = (i-1, 0)
    for j in range(1, m+1):
        dp[0][j] = j
        bt[0][j] = (0, j-1)

    for i in range(1, n+1):
        ri = R[i-1]
        for j in range(1, m+1):
            hj = H[j-1]
            cost_sub = 0 if ri == hj else 1
            # three options
            a = dp[i-1][j] + 1        # deletion (ri -> Îµ)
            b = dp[i][j-1] + 1        # insertion (Îµ -> hj)
            c = dp[i-1][j-1] + cost_sub  # substitution / match
            best = c
            prev = (i-1, j-1)
            if a < best:
                best, prev = a, (i-1, j)
            if b < best:
                best, prev = b, (i, j-1)
            dp[i][j] = best
            bt[i][j] = prev

    # backtrace path
    path: List[Tuple[Optional[str], Optional[str]]] = []
    i, j = n, m
    while i > 0 or j > 0:
        pi, pj = bt[i][j]
        if pi == i-1 and pj == j-1:
            # sub or match
            path.append((R[i-1], H[j-1]))
        elif pi == i-1 and pj == j:
            # deletion (ri -> Îµ)
            path.append((R[i-1], None))
        elif pi == i and pj == j-1:
            # insertion (Îµ -> hj)
            path.append((None, H[j-1]))
        else:
            # should not happen
            break
        i, j = pi, pj

    path.reverse()
    return path, dp[n][m]

# =============== Pretty helpers =========================
def _fmt_pct(x: float) -> str:
    return f"{x:.2%}"

def _bucket_order(keys):
    pref = {"clean": 0, "mid": 1, "noisy": 2, "unknown": 3}
    return sorted(keys, key=lambda k: (pref.get(k, 99), k))

# =============== Core evaluation ========================
def evaluate_detail_bucket(
    meta_path: str,
    bucket_field: str = "snr_bucket",
    ref_field: str = "text",
    hyp_field: str = "base_text",
    exclude_buckets: Optional[List[str]] = None,
    cer_strip_spaces: bool = True,
    conf_topn: int = 20,
    include_insert_delete: bool = True,
) -> Dict[str, Any]:
    with open(meta_path, "r", encoding="utf-8") as f:
        entries = json.load(f)
    if not isinstance(entries, list):
        raise ValueError("Manifest must be a JSON array.")

    if exclude_buckets is None:
        exclude_buckets = []

    # overall accumulators
    total_wer = total_cer = 0.0
    count = 0
    all_refs: List[str] = []
    all_hyps: List[str] = []

    # per-bucket
    b_sum_wer = defaultdict(float)
    b_sum_cer = defaultdict(float)
    b_count = defaultdict(int)
    b_refs = defaultdict(list)
    b_hyps = defaultdict(list)
    # confusion counters: Counter[(ref_char or 'â€', hyp_char or 'â€')]
    b_conf = defaultdict(Counter)
    # error totals per bucket for percentage denominator
    b_err_total = defaultdict(int)

    print(f"======>>>>>> Total entries: {len(entries)}")

    for e in entries:
        ref = _norm(e.get(ref_field, ""))
        hyp = _norm(e.get(hyp_field, ""))
        if not ref or not hyp:
            continue
        b = (e.get(bucket_field) or "unknown")
        if b in exclude_buckets:
            continue

        s_wer = jiwer_wer(ref, hyp)
        s_cer = _cer_value(ref, hyp, strip_spaces=cer_strip_spaces)

        total_wer += s_wer
        total_cer += s_cer
        count += 1
        all_refs.append(ref)
        all_hyps.append(hyp)

        b_sum_wer[b] += s_wer
        b_sum_cer[b] += s_cer
        b_count[b] += 1
        b_refs[b].append(ref)
        b_hyps[b].append(hyp)

        # confusions
        r = ref.replace(" ", "") if cer_strip_spaces else ref
        h = hyp.replace(" ", "") if cer_strip_spaces else hyp
        path, dist = _align_chars(r, h)
        b_err_total[b] += dist
        for rc, hc in path:
            if rc == hc:
                continue  # match
            ref_c = rc if rc is not None else "â€"  # epsilon
            hyp_c = hc if hc is not None else "â€"
            if not include_insert_delete and ("â€" in (ref_c, hyp_c)):
                continue
            b_conf[b][(ref_c, hyp_c)] += 1


        # print debug 
        if count % 10 == 0:
            print(f"Processed {count} samples: {b} | WER: {_fmt_pct(s_wer)} | CER: {_fmt_pct(s_cer)}")
    print("======>>>>>> Finished processing entries.")            

    if count == 0:
        return {"error": "No valid samples after filtering."}

    # overall metrics
    overall = {
        "num_samples": count,
        "avg_wer": total_wer / count,
        "avg_cer": total_cer / count,
        "global_wer": jiwer_wer(all_refs, all_hyps),
        "global_cer": _cer_value("".join(all_refs), "".join(all_hyps), cer_strip_spaces),
    }

    print(f"====== OVERALL =====")
    print(json.dumps(overall, ensure_ascii=False, indent=2))

    # per-bucket metrics

    ## print debug 
    by_bucket = {}
    for b in _bucket_order(b_count.keys()):
        n = b_count[b]
        if n == 0:
            continue
        avg_wer = b_sum_wer[b] / n
        avg_cer = b_sum_cer[b] / n
        glb_wer = jiwer_wer(b_refs[b], b_hyps[b])
        glb_cer = _cer_value("".join(b_refs[b]), "".join(b_hyps[b]), cer_strip_spaces)

        # top confusions
        conf_counter = b_conf[b]
        err_total = max(1, b_err_total[b])  # avoid div0
        top_items = conf_counter.most_common(conf_topn)
        top_conf = [
            {
                "ref": k[0],
                "hyp": k[1],
                "count": v,
                "ratio": v / err_total,  # tá»‰ lá»‡ trÃªn tá»•ng lá»—i (S+D+I) cá»§a bucket
            }
            for k, v in top_items
        ]

        by_bucket[b] = {
            "num_samples": n,
            "avg_wer": avg_wer,
            "avg_cer": avg_cer,
            "global_wer": glb_wer,
            "global_cer": glb_cer,
            "errors_total": b_err_total[b],
            "top_confusions": top_conf,
        }

        print(f"====== BUCKET [{b}] =====")
        print(json.dumps(by_bucket[b], ensure_ascii=False, indent=2))

    return {"overall": overall, "by_bucket": by_bucket}

# =============== Pretty print ===========================
def print_report(report: Dict[str, Any], conf_topn: int):
    if "error" in report:
        print(f"âš ï¸ {report['error']}")
        return
    ov = report["overall"]
    print("===== OVERALL =====")
    print(f"ğŸ“Š Tá»•ng sá»‘ máº«u: {ov['num_samples']}")
    print(f"ğŸ¯ WER trung bÃ¬nh: {_fmt_pct(ov['avg_wer'])}")
    print(f"ğŸ…² CER trung bÃ¬nh: {_fmt_pct(ov['avg_cer'])}")
    print(f"ğŸŒ WER toÃ n cá»¥c : {_fmt_pct(ov['global_wer'])}")
    print(f"ğŸŒ CER toÃ n cá»¥c : {_fmt_pct(ov['global_cer'])}")
    print()

    print("===== BY SNR BUCKET =====")
    for b, info in report["by_bucket"].items():
        print(f"[{b}]")
        print(f"  ğŸ“¦ samples        : {info['num_samples']}")
        print(f"  ğŸ¯ sample-avg WER : {_fmt_pct(info['avg_wer'])}")
        print(f"  ğŸ…² sample-avg CER : {_fmt_pct(info['avg_cer'])}")
        print(f"  ğŸŒ global WER     : {_fmt_pct(info['global_wer'])}")
        print(f"  ğŸŒ global CER     : {_fmt_pct(info['global_cer'])}")
        print(f"  âŒ total edits    : {info['errors_total']}")
        print(f"  --- Top {conf_topn} char confusions (ref â†’ hyp) ---")
        for i, it in enumerate(info["top_confusions"], 1):
            ref_c = it["ref"]
            hyp_c = it["hyp"]
            # hiá»ƒn thá»‹ epsilon Ä‘áº¹p
            ref_s = ref_c if ref_c != "â€" else "âˆ…"
            hyp_s = hyp_c if hyp_c != "â€" else "âˆ…"
            print(f"   {i:2d}. '{ref_s}' â†’ '{hyp_s}': {it['count']} ({_fmt_pct(it['ratio'])})")
        print()

# # =============== CLI ===================================
# def main():
#     ap = argparse.ArgumentParser(description="Eval WER/CER + char confusions per SNR bucket (JSON array manifest).")
#     ap.add_argument("--meta", required=True, help="Path to JSON array manifest")
#     ap.add_argument("--bucket-field", default="snr_bucket")
#     ap.add_argument("--ref-field", default="text")
#     ap.add_argument("--hyp-field", default="base_text")
#     ap.add_argument("--exclude-buckets", default="", help="Comma-separated buckets to exclude, e.g., 'noisy,unknown'")
#     ap.add_argument("--cer-strip-spaces", action="store_true", help="Compute CER after removing spaces")
#     ap.add_argument("--conf-topn", type=int, default=20, help="Top-N confusions to display per bucket")
#     ap.add_argument("--no-insert-delete", action="store_true",
#                     help="If set, exclude insertions/deletions (Îµ) from confusion table; only substitutions kept")
#     ap.add_argument("--out-json", default="", help="Optional: write full report JSON to this path")
#     args = ap.parse_args()

#     excl = [b.strip() for b in args.exclude_buckets.split(",") if b.strip()]
#     report = evaluate_report(
#         meta_path=args.meta,
#         bucket_field=args.bucket_field,
#         ref_field=args.ref_field,
#         hyp_field=args.hyp_field,
#         exclude_buckets=excl,
#         cer_strip_spaces=args.cer_strip_spaces,
#         conf_topn=args.conf_topn,
#         include_insert_delete=not args.no_insert_delete,
#     )
#     print_report(report, conf_topn=args.conf_topn)
#     if args.out_json:
#         with open(args.out_json, "w", encoding="utf-8") as f:
#             json.dump(report, f, ensure_ascii=False, indent=2)

# if __name__ == "__main__":
#     main()


# ======== EVALUATE BUCKETS ========

#!/usr/bin/env python3
import json
import re
from collections import defaultdict
from typing import List, Dict, Any, Optional
from jiwer import wer
import editdistance

# ---------- text normalization (nháº¹) ----------
def _norm(s: str) -> str:
    s = (s or "").lower().strip()
    # chuáº©n hoÃ¡ khoáº£ng tráº¯ng
    s = re.sub(r"\s+", " ", s)
    return s

def _cer(ref: str, hyp: str, strip_spaces: bool = True) -> float:
    """
    CER: (S + D + I) / |ref|
    - strip_spaces=True: loáº¡i bá» khoáº£ng tráº¯ng trÆ°á»›c khi so kÃ½ tá»± (khuyáº¿n nghá»‹ cho TViá»‡t)
    """
    if strip_spaces:
        ref = ref.replace(" ", "")
        hyp = hyp.replace(" ", "")
    ref_chars = list(ref)
    hyp_chars = list(hyp)
    dist = editdistance.eval(ref_chars, hyp_chars)
    return dist / max(1, len(ref_chars))

def evaluate_from_meta_by_bucket(
    meta_path: str,
    bucket_field: str = "snr_bucket",
    ref_field: str = "text",
    hyp_field: str = "base_text",
    exclude_buckets: Optional[List[str]] = None,
    cer_strip_spaces: bool = True,
):
    """
    Äá»c manifest JSON array vÃ  in:
      - OVERALL: sample-avg WER/CER + global WER/CER
      - BY BUCKET (snr): sample-avg & global cho tá»«ng bucket

    Args:
        meta_path: Ä‘Æ°á»ng dáº«n manifest JSON (array)
        bucket_field: trÆ°á»ng bucket (vd 'snr_bucket')
        ref_field: trÆ°á»ng ref
        hyp_field: trÆ°á»ng hyp (ASR output)
        exclude_buckets: danh sÃ¡ch bucket cáº§n loáº¡i (vd ['noisy'])
        cer_strip_spaces: CER tÃ­nh theo kÃ½ tá»± sau khi bá» khoáº£ng tráº¯ng
    """
    with open(meta_path, "r", encoding="utf-8") as f:
        entries: List[Dict[str, Any]] = json.load(f)

    if exclude_buckets is None:
        exclude_buckets = []

    # Tá»•ng thá»ƒ
    total_wer = 0.0
    total_cer = 0.0
    count = 0
    all_refs: List[str] = []
    all_hyps: List[str] = []

    # Theo bucket
    bucket_refs: Dict[str, List[str]] = defaultdict(list)
    bucket_hyps: Dict[str, List[str]] = defaultdict(list)
    bucket_sum_wer: Dict[str, float] = defaultdict(float)
    bucket_sum_cer: Dict[str, float] = defaultdict(float)
    bucket_count: Dict[str, int] = defaultdict(int)

    for e in entries:
        ref = _norm(e.get(ref_field, ""))
        hyp = _norm(e.get(hyp_field, ""))
        if not ref or not hyp:
            continue

        b = (e.get(bucket_field) or "unknown")
        if b in exclude_buckets:
            continue

        s_wer = wer(ref, hyp)
        s_cer = _cer(ref, hyp, strip_spaces=cer_strip_spaces)

        total_wer += s_wer
        total_cer += s_cer
        count += 1
        all_refs.append(ref)
        all_hyps.append(hyp)

        bucket_sum_wer[b] += s_wer
        bucket_sum_cer[b] += s_cer
        bucket_count[b] += 1
        bucket_refs[b].append(ref)
        bucket_hyps[b].append(hyp)

    if count == 0:
        print("âš ï¸ No valid samples after filtering.")
        return

    avg_wer = total_wer / count
    avg_cer = total_cer / count
    global_wer = wer(all_refs, all_hyps)
    # global CER: ná»‘i chuá»—i (hoáº·c cÃ³ thá»ƒ gá»™p theo list kÃ½ tá»±)
    global_cer = _cer("".join(all_refs), "".join(all_hyps), strip_spaces=cer_strip_spaces)

    print("===== OVERALL =====")
    print(f"ğŸ“Š Tá»•ng sá»‘ máº«u: {count}")
    print(f"ğŸ¯ WER trung bÃ¬nh: {avg_wer:.2%}")
    print(f"ğŸ…² CER trung bÃ¬nh: {avg_cer:.2%}")
    print(f"ğŸŒ WER toÃ n cá»¥c : {global_wer:.2%}")
    print(f"ğŸŒ CER toÃ n cá»¥c : {global_cer:.2%}")
    print()

    # Thá»© tá»± in bucket: clean, mid, noisy, unknown, rá»“i alphabet
    preferred = {"clean": 0, "mid": 1, "noisy": 2, "unknown": 3}
    buckets = list(bucket_count.keys())
    buckets.sort(key=lambda x: (preferred.get(x, 99), x))

    print("===== BY SNR BUCKET =====")
    for b in buckets:
        n = bucket_count[b]
        if n == 0:
            continue
        b_avg_wer = bucket_sum_wer[b] / n
        b_avg_cer = bucket_sum_cer[b] / n
        b_glb_wer = wer(bucket_refs[b], bucket_hyps[b])
        b_glb_cer = _cer("".join(bucket_refs[b]), "".join(bucket_hyps[b]), strip_spaces=cer_strip_spaces)
        print(f"[{b}]")
        print(f"  ğŸ“¦ samples        : {n}")
        print(f"  ğŸ¯ sample-avg WER : {b_avg_wer:.2%}")
        print(f"  ğŸ…² sample-avg CER : {b_avg_cer:.2%}")
        print(f"  ğŸŒ global WER     : {b_glb_wer:.2%}")
        print(f"  ğŸŒ global CER     : {b_glb_cer:.2%}")
        print()





# ======== EVALUATE ========

import json
from jiwer import wer
from pathlib import Path

def evaluate_from_meta(meta_path: str):
    with open(meta_path, "r", encoding="utf-8") as f:
        entries = json.load(f)

    total_wer = 0.0
    count = 0
    all_refs = []
    all_preds = []

    for entry in entries:
        try: 
            ref = entry.get("text", "").strip().lower()
            hyp = entry.get("base_text", "").strip().lower()
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi Ä‘á»c entry {entry}: {e}")
            # print stack trace
            import traceback
            traceback.print_exc()
            # print value:
            print(f"  - ref: {ref}")
            print(f"  - hyp: {hyp}")
            exit
        if not ref or not hyp:
            continue

        sample_wer = wer(ref, hyp)
        total_wer += sample_wer
        count += 1

        all_refs.append(ref)
        all_preds.append(hyp)

    if count == 0:
        print("âš ï¸ No valid samples found.")
        return

    avg_wer = total_wer / count
    global_wer = wer(all_refs, all_preds)

    print(f"ğŸ“Š Tá»•ng sá»‘ máº«u: {count}")
    print(f"ğŸ¯ WER trung bÃ¬nh (sample avg): {avg_wer:.2%}")
    print(f"ğŸŒ WER toÃ n cá»¥c   (global):     {global_wer:.2%}")



from jiwer import wer
from chunkformer_vpb.model_utils import decode_long_form, decode_aed_long_form, get_default_args
from jiwer import wer

def evaluate(model, tokenizer, loader, cfg, device, mode="ctc", eval_ratio=1.0):
    if loader is None:
        print("ğŸš« No eval data found. Skipping evaluation.")
        return

    model.eval()
    total_wer, count = 0.0, 0
    all_refs, all_preds = [], []

    args = get_default_args()
    args.chunk_size = cfg.chunk.chunk_size
    args.left_context_size = cfg.chunk.left_context_size
    args.right_context_size = cfg.chunk.right_context_size
    args.total_batch_duration = cfg.chunk.total_batch_duration

    char_dict = tokenizer.vocab

    max_samples = int(len(loader.dataset) * eval_ratio)
    processed_samples = 0

    start_eval_time = time.time()  # â±ï¸ Tá»•ng thá»i gian evaluate
    total_decode_time = 0.0

    with torch.no_grad():
        for loader_item in loader:
            feats, feat_lens, toks, tok_lens = loader_item[0:4]  # Láº¥y 4 pháº§n Ä‘áº§u tiÃªn
            feats, feat_lens = feats.to(device), feat_lens.to(device)
            toks,  tok_lens  = toks.to(device),  tok_lens.to(device)

            for i in range(feats.size(0)):
                if processed_samples >= max_samples:
                    break

                x = feats[i].unsqueeze(0)
                y = toks[i].unsqueeze(0)
                y_lens = tok_lens[i].item()

                ref_ids = y[0, :y_lens].tolist()
                ref_text = tokenizer.decode_ids(ref_ids)

                decode_start = time.time()
                if mode == "ctc":
                    pred_text = decode_long_form(x, model, char_dict, args, device)
                elif mode == "aed":
                    _, pred_text = decode_aed_long_form(x, model, char_dict, args, device)
                else:
                    raise ValueError(f"Unknown mode: {mode}")
                decode_duration = time.time() - decode_start
                total_decode_time += decode_duration

                ref_text = ref_text.lower()
                pred_text = pred_text.lower()

                total_wer += wer(ref_text, pred_text)
                count += 1
                processed_samples += 1

                all_refs.append(ref_text)
                all_preds.append(pred_text)

            if processed_samples >= max_samples:
                break

    total_eval_time = time.time() - start_eval_time

    if count == 0:
        print("âš ï¸ No samples evaluated.")
        return

    avg_wer = total_wer / count
    global_wer = wer(all_refs, all_preds)
    avg_decode_time = total_decode_time / count

    print(f"ğŸ¯ Dev WER ({mode.upper()}): {avg_wer:.2%}")
    print(f"ğŸŒ Global WER           : {global_wer:.2%}")
    print(f"ğŸ•’ Evaluate time: {total_eval_time:.2f}s "
          f"(avg decode/sample: {avg_decode_time:.2f}s)")
    model.train()


####################################################

import csv
import torch
import time
from jiwer import wer
from chunkformer_vpb.model_utils import decode_long_form, decode_aed_long_form, get_default_args

def evaluate_debug(model, tokenizer, loader, cfg, device,
                   output_csv="debug_eval_output.csv", mode="ctc", eval_ratio=1.0):
    """
    ÄÃ¡nh giÃ¡ chi tiáº¿t Ä‘á»ƒ debug sá»± khÃ¡c biá»‡t giá»¯a:
    - text (label json gá»‘c)
    - gold_corrected (label thá»§ cÃ´ng)
    - pred_old (output cÅ© cá»§a model)
    - pred_new (output má»›i tá»« mÃ´ hÃ¬nh hiá»‡n táº¡i)
    """

    if loader is None:
        print("ğŸš« No eval data found. Skipping.")
        return

    model.eval()
    args = get_default_args()
    args.chunk_size = cfg.chunk.chunk_size
    args.left_context_size = cfg.chunk.left_context_size
    args.right_context_size = cfg.chunk.right_context_size
    args.total_batch_duration = cfg.chunk.total_batch_duration

    char_dict = tokenizer.vocab
    max_samples = int(len(loader.dataset) * eval_ratio)
    processed_samples = 0
    total_decode_time = 0.0

    rows = []

    with torch.no_grad():
        for batch in loader:
            feats, feat_lens, toks, tok_lens, utt_ids, golds, preds = batch
            feats, feat_lens = feats.to(device), feat_lens.to(device)
            toks, tok_lens = toks.to(device), tok_lens.to(device)

            for i in range(feats.size(0)):
                if processed_samples >= max_samples:
                    break

                x = feats[i].unsqueeze(0)
                y = toks[i].unsqueeze(0)
                y_lens = tok_lens[i].item()

                # text tá»« json (label huáº¥n luyá»‡n cÅ©)
                ref_ids = y[0, :y_lens].tolist()
                text_json = tokenizer.decode_ids(ref_ids).lower()

                # decode tá»« model hiá»‡n táº¡i
                decode_start = time.time()
                if mode == "ctc":
                    pred_new = decode_long_form(x, model, char_dict, args, device)
                elif mode == "aed":
                    _, pred_new = decode_aed_long_form(x, model, char_dict, args, device)
                else:
                    raise ValueError(f"Unknown mode: {mode}")
                total_decode_time += time.time() - decode_start

                pred_new = pred_new.lower()
                gold = (golds[i] or "").lower()
                pred_old = (preds[i] or "").lower()
                utt_id = utt_ids[i]

                r = {
                    "utt_id": utt_id,
                    "text_json": text_json,
                    "gold_corrected": gold,
                    "pred_old": pred_old,
                    "pred_new": pred_new,
                    "wer_json_vs_new": wer(text_json, pred_new),
                    "wer_gold_vs_new": wer(gold, pred_new) if gold else None,
                    "wer_gold_vs_old": wer(gold, pred_old) if gold and pred_old else None,
                }

                rows.append(r)
                print(f"ğŸ” Sample {processed_samples + 1}/{max_samples} - "
                      f"utt_id: {utt_id}, WER(json vs new): {r['wer_json_vs_new']:.2%}, "
                      f"WER(gold vs new): {r['wer_gold_vs_new']:.2%} "
                      f"WER(gold vs old): {r['wer_gold_vs_old']:.2%}")
                print(f"  - text_json: {text_json}")
                print(f"  - gold_corrected: {gold}")
                print(f"  - pred_old: {pred_old}")
                print(f"  - pred_new: {pred_new}")
                print("-" * 80)



                processed_samples += 1
            if processed_samples >= max_samples:
                break

    # Ghi ra CSV Ä‘á»ƒ phÃ¢n tÃ­ch
    # create folder 
    os.makedirs(cfg.training.checkpoint_dir, exist_ok=True)
    output_path = os.path.join(cfg.training.checkpoint_dir, output_csv)
    with open(output_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)

    print(f"âœ… Saved debug result to {output_path}")
    print(f"ğŸ” Evaluated {processed_samples} samples")
    print(f"ğŸ•’ Avg decode/sample: {total_decode_time / processed_samples:.2f}s")



# ======== MAIN ========
if __name__ == "__main__":
    train()
