# chunkformer_vpb/inference/main_infer.py

import torch
import argparse
from torchaudio.compliance.kaldi import fbank
from jiwer import wer
from chunkformer_vpb.decode import init, load_audio
from chunkformer_vpb.model.utils.ctc_utils import get_output_with_timestamps
from chunkformer_vpb.model.utils.mask import make_pad_mask
# ==================== ARG PARSE ====================
def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_checkpoint', type=str, required=True,
                        help='Path to model checkpoint folder (e.g. chunkformer-large-vie)')
    parser.add_argument('--audio_path', type=str, required=True,
                        help='Path to .wav audio file to transcribe')
    parser.add_argument('--label_text', type=str, default=None,
                        help='(Optional) Ground truth text for WER comparison')
    parser.add_argument('--chunk_size', type=int, default=64)
    parser.add_argument('--left_context_size', type=int, default=128)
    parser.add_argument('--right_context_size', type=int, default=128)
    parser.add_argument('--total_batch_duration', type=int, default=1800)  # in ms
    args = parser.parse_args()
    return args

def get_default_args():
    return argparse.Namespace(
        model_checkpoint="chunkformer-large-vie",
        audio_path="samples/test.wav",
        label_text=None,
        chunk_size=64,
        left_context_size=128,
        right_context_size=128,
        total_batch_duration=1800
    )

# ==================== MODEL ====================
def load_model_only(model_checkpoint="../chunkformer-large-vie", device=None):
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, char_dict = init(model_checkpoint, device)
    model.eval()
    return model, char_dict

def dump_module_structure(module: torch.nn.Module, output_path: str, title: str = ""):
    with open(output_path, "w", encoding="utf-8") as f:
        if title:
            f.write(f"==== {title} ====\n\n")
        f.write(str(module))


# ==================== PREPROCESS ====================
def prepare_input_file(audio_path, device):
    waveform = load_audio(audio_path)
    if waveform.dim() == 1:
        waveform = waveform.unsqueeze(0)
    if waveform.abs().max() <= 1.0:
        waveform = waveform * 32768.0
    waveform = waveform.clamp(-32768, 32767).to(device)

    feats = fbank(
        waveform,
        num_mel_bins=80,
        frame_length=25,
        frame_shift=10,
        dither=0.0,
        energy_floor=0.0,
        sample_frequency=16000
    ).unsqueeze(0).to(device)
    return feats

@torch.no_grad()
def decode_aed_long_form(xs, model, char_dict, args, device):
    chunk_size = args.chunk_size
    left_context_size = args.left_context_size
    right_context_size = args.right_context_size
    subsampling_factor = model.encoder.embed.subsampling_factor
    conv_kernel_size = model.encoder.cnn_module_kernel
    conv_lorder = conv_kernel_size // 2
    num_blocks = model.encoder.num_blocks
    hidden_dim = model.encoder._output_size
    attention_heads = model.encoder.attention_heads

    max_len = int(args.total_batch_duration // 0.01) // 2
    multiply_n = max_len // chunk_size // subsampling_factor
    truncated_context_size = chunk_size * multiply_n
    rel_right_context_size = (
        right_context_size + max(chunk_size, right_context_size) * (num_blocks - 1)
    ) * subsampling_factor

    # Init cache
    offset = torch.zeros(1, dtype=torch.int, device=device)
    att_cache = torch.zeros((num_blocks, left_context_size, attention_heads, hidden_dim * 2 // attention_heads), device=device)
    cnn_cache = torch.zeros((num_blocks, hidden_dim, conv_lorder), device=device)

    encoder_outs_chunks = []
    encoder_lens_total = 0

    for idx in range(0, xs.shape[1], truncated_context_size * subsampling_factor):
        start = truncated_context_size * subsampling_factor * idx
        end = min(start + truncated_context_size * subsampling_factor + 7, xs.shape[1])

        x = xs[:, start:end + rel_right_context_size]
        x_len = torch.tensor([x[0].shape[0]], dtype=torch.int, device=device)

        encoder_outs, encoder_lens, _, att_cache, cnn_cache, offset = model.encoder.forward_parallel_chunk(
            xs=x, 
            xs_origin_lens=x_len,
            chunk_size=chunk_size,
            left_context_size=left_context_size,
            right_context_size=right_context_size,
            att_cache=att_cache,
            cnn_cache=cnn_cache,
            truncated_context_size=truncated_context_size,
            offset=offset
        )

        encoder_outs = encoder_outs[:, :encoder_lens]
        encoder_outs_chunks.append(encoder_outs)
        encoder_lens_total += encoder_outs.shape[1]

        if start + rel_right_context_size >= xs.shape[1]:
            break

    # Cat all chunk outputs
    encoder_outs_full = torch.cat(encoder_outs_chunks, dim=1)
    encoder_mask = torch.ones(1, 1, encoder_outs_full.shape[1], dtype=torch.bool, device=device)

    # Decode AED
    pred_token_ids, _ = model.decode_aed(encoder_outs_full, encoder_mask, maxlen=100)
    pred_ids = pred_token_ids[0].tolist()  # ƒê·ªïi t√™n cho ng·∫Øn g·ªçn

    # Raw vs Clean
    raw = transcript_raw(pred_ids, char_dict)
    clean = transcript_clean(pred_ids, char_dict)

    print(f"üì£ AED RAW   : {raw}")
    print(f"üì£ AED CLEAN : {clean}")

    return raw, clean

def transcript_raw(token_ids, char_dict):
    """
    Convert list of token IDs ‚Üí raw string (gi·ªØ nguy√™n c√°c k√Ω t·ª± subword, <sos>, <eos>, blank).
    Args:
        token_ids: List[int]
        char_dict: Dict[int, str]
    Returns:
        raw_transcript: str
    """
    return "".join(char_dict.get(idx, "") for idx in token_ids)


def transcript_clean(token_ids, char_dict,
                     sos_token="<sos>", eos_token="<eos>",
                     blank_token="_"):
    """
    T·∫°o b·∫£n clean:
      - Lo·∫°i b·ªè sos/eos/blank
      - Chuy·ªÉn k√Ω t·ª± subword (‚Äò‚ñÅ‚Äô) th√†nh kho·∫£ng tr·∫Øng
    Args:
        token_ids: List[int]
        char_dict: Dict[int, str]
        sos_token: chu·ªói bi·ªÉu di·ªÖn <sos> trong char_dict
        eos_token: chu·ªói bi·ªÉu di·ªÖn <eos> trong char_dict
        blank_token: k√Ω t·ª± blank (th∆∞·ªùng l√† "_")
    Returns:
        clean_transcript: str
    """
    parts = []
    for idx in token_ids:
        c = char_dict.get(idx, "")
        if c in (sos_token, eos_token, blank_token):
            continue
        if c.startswith("‚ñÅ"):
            # subword marker ‚Üí space + ph·∫ßn c√≤n l·∫°i
            parts.append(" " + c.lstrip("‚ñÅ"))
        else:
            parts.append(c)
    return "".join(parts).strip()


@torch.no_grad()  # Kh√¥ng c·∫ßn gradient v√¨ ƒëang trong ch·∫ø ƒë·ªô inference
def decode_long_form(xs, model, char_dict, args, device):
    """
    Input:
        xs: Tensor d·∫°ng [1, T_raw, 80], ƒë·∫∑c tr∆∞ng √¢m thanh (FBank)
        model: m√¥ h√¨nh ASR (ChunkFormer + CTC)
        char_dict: t·ª´ ƒëi·ªÉn √°nh x·∫° ID ‚Üí k√Ω t·ª±
        args: c√°c hyperparameter decode
        device: cuda ho·∫∑c cpu
    Output:
        full_text: chu·ªói k√Ω t·ª± ƒë√£ decode
    """

    def get_max_input_context(c, r, n):
        # T√≠nh t·ªïng s·ªë frame ph·∫£i c·∫ßn nh√¨n b√™n ph·∫£i
        return r + max(c, r) * (n - 1)

    # === L·∫•y config ===
    '''
    üéØ I. Gi·∫£i th√≠ch kh√°i ni·ªám chunk trong ASR
    üß† 1. V·∫•n ƒë·ªÅ:
    M√¥ h√¨nh ASR truy·ªÅn th·ªëng c·∫ßn to√†n b·ªô ƒëo·∫°n audio ƒë·ªÉ decode ‚Üí ‚ùå kh√¥ng d√πng ƒë∆∞·ª£c cho realtime.
    C√†ng d√†i, c√†ng t·ªën RAM v√† kh√¥ng th·ªÉ x·ª≠ l√Ω online.

    ‚ö° Gi·∫£i ph√°p: Chunk-based decoding
    Chia ƒë·∫∑c tr∆∞ng √¢m thanh ƒë·∫ßu v√†o (xs) th√†nh c√°c kh·ªëi nh·ªè c√≥ ƒë·ªô d√†i c·ªë ƒë·ªãnh.
    M·ªói kh·ªëi (chunk) ƒë∆∞·ª£c x·ª≠ l√Ω ri√™ng bi·ªát, nh∆∞ng v·∫´n c√≥ context tr√°i/ph·∫£i ƒë·ªÉ m√¥ h√¨nh hi·ªÉu ng·ªØ c·∫£nh.
    '''
    chunk_size = args.chunk_size                      # S·ªë frame m·ªói chunk sau subsampling
    left_context_size = args.left_context_size        # S·ªë frame tr√°i cho attention
    right_context_size = args.right_context_size      # S·ªë frame ph·∫£i cho attention
    subsampling_factor = model.encoder.embed.subsampling_factor  # th∆∞·ªùng = 4
    conv_lorder = model.encoder.cnn_module_kernel // 2  # lookahead c·ªßa conv (th∆∞·ªùng = 7)
    '''
    | Term                     | √ù nghƒ©a                                                           |
    | ------------------------ | ----------------------------------------------------------------- |
    | `chunk_size`             | S·ªë frame ch√≠nh c·∫ßn decode trong m·ªói kh·ªëi                          |
    | `left_context_size`      | S·ªë frame b√™n tr√°i ƒë∆∞·ª£c d√πng l√†m ng·ªØ c·∫£nh                          |
    | `right_context_size`     | S·ªë frame b√™n ph·∫£i (look-ahead) h·ªó tr·ª£ m√¥ h√¨nh                     |
    | `truncated_context_size` | T·ªïng s·ªë frame c√≥ th·ªÉ x·ª≠ l√Ω trong 1 chunk theo gi·ªõi h·∫°n t√†i nguy√™n |

    '''

    # === L·∫•y config t·ª´ args v√† model ===
    chunk_size = args.chunk_size
    left_context_size = args.left_context_size
    right_context_size = args.right_context_size
    subsampling_factor = model.encoder.embed.subsampling_factor
    conv_kernel_size = model.encoder.cnn_module_kernel
    conv_lorder = conv_kernel_size // 2
    num_blocks = model.encoder.num_blocks
    hidden_dim = model.encoder._output_size
    attention_heads = model.encoder.attention_heads

    # print("========== üîß CONFIG SUMMARY ==========")
    # print(f"  chunk_size               = {chunk_size}")
    # print(f"  left_context_size        = {left_context_size}")
    # print(f"  right_context_size       = {right_context_size}")
    # print(f"  subsampling_factor       = {subsampling_factor}")
    # print(f"  conv_kernel_size         = {conv_kernel_size}")
    # print(f"  conv_lorder              = {conv_lorder}")
    # print(f"  num_blocks               = {num_blocks}")
    # print(f"  hidden_dim (_output_size)= {hidden_dim}")
    # print(f"  attention_heads          = {attention_heads}")
    # print(f"  Input shape              = {xs.shape}")  # [1, T_raw, 80]
    # print("=======================================\n")

    # === T√≠nh ƒë·ªô d√†i th·ª±c t·∫ø m·ªói chunk c√≥ th·ªÉ x·ª≠ l√Ω trong GPU ===
    max_len = int(args.total_batch_duration // 0.01) // 2  # T·ªïng s·ªë frame (tr∆∞·ªõc subsample)
    multiply_n = max_len // chunk_size // subsampling_factor
    truncated_context_size = chunk_size * multiply_n  # s·ªë frame gi·ªØ l·∫°i m·ªói chunk sau encoder

    # === T√≠nh t·ªïng context ph·∫£i (ph·ª•c v·ª• attention + conv) sau subsample ===
    rel_right_context_size = get_max_input_context(
        chunk_size, max(right_context_size, conv_lorder), model.encoder.num_blocks
    ) * subsampling_factor


    # print("üßÆ CALCULATED CONTEXT INFO")
    # print(f"  total_batch_duration     = {args.total_batch_duration}")
    # print(f"  max_len (frame count)    = {max_len}")
    # print(f"  multiply_n               = {multiply_n}")
    # print(f"  truncated_context_size   = {truncated_context_size}")
    # print(f"  rel_right_context_size   = {rel_right_context_size}\n")
    

    '''
    üß± B∆∞·ªõc 1: T√°ch theo chunk
    text
    Copy code
    üî£ Full Input xs: [1, 1000, 80]
        ‚îÇ
        ‚îú‚îÄ‚îÄ> Chunk 0: [start=0 : end=256] + rel_right_context=64 ‚Üí [1, 320, 80]
        ‚îÇ
        ‚îú‚îÄ‚îÄ> Chunk 1: [start=256 : end=512] + rel_right_context=64 ‚Üí [1, 320, 80]
        ‚îÇ
        ‚îî‚îÄ‚îÄ> ...
    M·ªói chunk l·∫•y ch√≠nh x√°c chunk_size * subsampling_factor * N

    Ph√≠a sau c·ªông th√™m rel_right_context_size ƒë·ªÉ m√¥ h√¨nh c√≥ look-ahead  
    '''

    # === Init cache: cho attention & conv, l∆∞u tr·∫°ng th√°i gi·ªØa c√°c chunk ===
    offset = torch.zeros(1, dtype=torch.int, device=device)
    num_blocks = model.encoder.num_blocks
    att_cache = torch.zeros(
                    (
                        num_blocks, 
                        left_context_size,
                        model.encoder.attention_heads,
                        model.encoder._output_size * 2 // model.encoder.attention_heads
                    ), 
                    device=device
                )
    cnn_cache = torch.zeros(
                    (
                        num_blocks, 
                        model.encoder._output_size, 
                        conv_lorder
                    ), 
                    device=device
                )

    # === Danh s√°ch ch·ª©a k·∫øt qu·∫£ d·ª± ƒëo√°n theo frame (ID nh√£n) c·ªßa t·ª´ng chunk ===
    framewise_token_ids_chunks = []

    # === Duy·ªát qua to√†n b·ªô input xs (T_raw frame) theo t·ª´ng chunk ===
    num_chunks = 0
    for idx in range(0, xs.shape[1], truncated_context_size * subsampling_factor):
        start = truncated_context_size * subsampling_factor * idx
        end = min(start + truncated_context_size * subsampling_factor + 7, xs.shape[1])

        # x: [1, T_chunk + context_right, 80]
        x = xs[:, start:end + rel_right_context_size]
        x_len = torch.tensor([x[0].shape[0]], dtype=torch.int, device=device)  # [1]

        # === Forward encoder theo chunk (streaming mode) ===
        '''
        üì§ B∆∞·ªõc 3: Ch·∫°y CTC tr√™n chunk_output
        text
        Copy code
        encoder_out: [1, 64, 512]
            ‚îÇ
            ‚îî‚îÄ‚îÄ> CTC Linear ‚Üí logits: [1, 64, vocab]
                        ‚Üì
                    Argmax ‚Üí [64]  ‚Üê framewise_token_ids (ID nh√£n)
        M·ªói chunk ‚Üí chu·ªói ID token d·ª± ƒëo√°n cho 64 frame.
        '''
        # print(f"\nüì¶ Chunk {num_chunks}")
        # print(f"  Input chunk frame idx: {start} ‚Üí {end}")
        # print(f"  Input x shape         : {x.shape}")
        # print(f"  x_len                 : {x_len.tolist()}")


        encoder_outs, encoder_lens, _, att_cache, cnn_cache, offset = model.encoder.forward_parallel_chunk(
            xs=x, 
            xs_origin_lens=x_len,
            chunk_size=chunk_size,
            left_context_size=left_context_size,
            right_context_size=right_context_size,
            att_cache=att_cache,
            cnn_cache=cnn_cache,
            truncated_context_size=truncated_context_size,
            offset=offset
        )
        # encoder_outs: [1, T_out, 512], encoder_lens: [1]
        # print(f"  encoder_outs shape    : {encoder_outs.shape}")
        # print(f"  encoder_lens          : {encoder_lens.tolist()}")

        # === Reshape v√† c·∫Øt b·ªè ph·∫ßn rel_right_context th·ª´a ===
        encoder_outs = encoder_outs.reshape(1, -1, encoder_outs.shape[-1])[:, :encoder_lens]
        if chunk_size * multiply_n * subsampling_factor * idx + rel_right_context_size < xs.shape[1]:
            encoder_outs = encoder_outs[:, :truncated_context_size]  # gi·ªØ l·∫°i ph·∫ßn ch√≠nh

        offset = offset - encoder_lens + encoder_outs.shape[1]

        # === Ch·∫°y CTC forward: t·ª´ [1, T_out, 512] ‚Üí [T_out] token_id ===
        framewise_ids = model.encoder.ctc_forward(encoder_outs).squeeze(0)  # shape: [T_out]
        # print(f"  framewise_ids shape   : {framewise_ids.shape}")
        # print(f"  framewise_ids (first 10): {framewise_ids.tolist()[:10]}")
        
        framewise_token_ids_chunks.append(framewise_ids)
        num_chunks += 1

        # === N·∫øu ƒë√£ x·ª≠ l√Ω xong to√†n b·ªô input th√¨ d·ª´ng ===
        if start + rel_right_context_size >= xs.shape[1]:
            break

    # === G·ªôp c√°c chunk l·∫°i: [T_total]
    '''
    üì¶ B∆∞·ªõc 4: Gom l·∫°i to√†n b·ªô c√°c chunk
    text
    Copy code
    framewise_token_ids_chunks = [
        [id_1, id_2, ..., id_64],       ‚Üê Chunk 0
        [id_65, ..., id_128],           ‚Üê Chunk 1
        ...
    ]
    ‚Üí cat ‚Üí full_framewise_ids: [total_T]
    '''
    full_framewise_ids = torch.cat(framewise_token_ids_chunks)
    print(f"\nüìä Total full_framewise_ids shape: {full_framewise_ids.shape}")
    print(f"    Sample token IDs (first 20): {full_framewise_ids.tolist()[:20]}")

    # === Decode theo CTC: lo·∫°i blank, duplicate, convert ID ‚Üí char, g√°n timestamp ===
    '''
    üîÑ B∆∞·ªõc 5: Decode CTC
    text
    Copy code
    full_framewise_ids = [b, b, a, a, _, b, b, _]
            ‚Üì
        Remove dup: [b, a, _, b]
        Remove blank: [b, a, b]
        Map ID ‚Üí char
        Add timestamps per chunk
    '''
    decoded_segments = get_output_with_timestamps([full_framewise_ids], char_dict)[0]
    print("\nüìù Decoded segments (first 3):")
    for seg in decoded_segments[:3]:
        print(f"  ‚Üí {seg}")

    # === Gh√©p t·∫•t c·∫£ l·∫°i th√†nh c√¢u ho√†n ch·ªânh ===
    final_transcript = " ".join([item["decode"] for item in decoded_segments])
    print(f"\n‚úÖ Final transcript: {final_transcript}")
    return final_transcript


# ==================== MAIN ====================
def run_inference(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model, char_dict = init(args.model_checkpoint, device)
    model.eval()

    feats = prepare_input_file(args.audio_path, device)
    decoded = decode_long_form(feats, model, char_dict, args, device)

    print("üü¢ Prediction     :", decoded)
    if args.label_text:
        print("üîµ Ground Truth   :", args.label_text)
        print("‚ùå WER            :", wer(args.label_text.lower(), decoded.lower()))

if __name__ == '__main__':
    args = get_args()
    run_inference(args)

'''
python -m chunkformer_vpb.model_utils \
  --model_checkpoint ../chunkformer-large-vie \
  --audio_path ../debug_wavs/sample_00.wav \
  --label_text "n·ª≠a v√≤ng tr√°i ƒë·∫•t h∆°n b·∫£y nƒÉm"

'''
